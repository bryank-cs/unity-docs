{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Introduction to Unity \u00b6 Welcome to Unity! The Unity cluster is a collaborative, multi-institutional high-performance computing cluster located at the Massachusetts Green High Performance Computing Center (MGHPCC) . The cluster is under active development and supports primarily research activities. Partnering institutions currently include UMass Amherst, UMass Dartmouth, and University of Rhode Island. Accessing Unity \u00b6 Faculty/PI's: If you are a Faculty member or Principal Investigator (PI) , please create an account on the Unity main page . Use your campus NetID and password to sign up for a Unity account. Once your account is created, go to Account Settings and request PI status by clicking the \"Request PI account\" button. Once your account is approved you will be able to approve adding additional users to your PI group in the \u201cMy Users\u201d button on the side menu. Adding users will grant them access to your PI /work directory. Users in your group need to request access to join your PI group. You cannot create accounts for your users. Students: In order to use Unity you must be working with a PI and they must grant you access to their PI group. Create an account on the Unity main page . Click \"Login/Request Account\". Click on your institution or search for it via the search bar. Once your account is created you will need to assign a PI to it. To do this, go to \"My PIs\" using the left side menu and click the large \"+\" button. Follow the prompts to generate your unique SSH key. You can search for your PI by name. Once found, click on \u201cSend Request\u201d. After your PI approves your request, you will have access to Unity and your PI\u2019s project code and data stored in Unity. You can view, join, or leave a PI group by clicking \"My PIs\". Note: If you are a student, do not request a PI account . This will slow the process down. You should assign a PI to your account by clicking the \"+\" button. Your PI will then be able to approve your account. Something not working for you? Send an email to hpc@umass.edu with as much detail as you can provide to open a support ticket. Need additional help? We offer office hours every week on Tuesdays 2:30-4 PM on Zoom . Be sure to check the cluster notes page for up-to-date information on any canceled/delayed office hours. Need expert help using the cluster and optimizing your code? We encourage you to schedule an appointment with one of our experienced HPC facilitators. Send an email to hpc@umass.edu and request an HPC facilitator consultation. Connecting to the cluster \u00b6 You can connect to Unity in two ways, an SSH connection (the standard Linux console), or an instance of JupyterHub: JupyterHub is the easiest to get up and going. To work with JupyterHub, it's a good idea to get aquainted with roughly how demanding the job you're uploading is though. You just need to be familiar with how roughly how many of each resource you will need (Image below). Most of the time you will only ever need a single CPU or GPU, but if you have terabytes of data to analyze, then you should probably consider getting multiple GPUs and CPUs. When connecting the portal, click on JupyterHub tab located at the bottom of the options list on the left side of the window. This will take you to the JupyterHub for Unity, which looks like this: You will be asked to select what computer resources you want/need for the job you want to upload. Once you attempt to spawn your notebook and resources become available, you will be able to use JupyterHub as if it is running on your own computer. SSH is the more traditional method of using an HPC cluster. You will connect to the login node of unity, and you will be responsible for starting your own jobs. This can be more useful than JupyterHubs for jobs that last a long time and must be left unattended, or to have much more refined control over the resources allocated for your job. Requesting Resources \u00b6 If you are on an SSH connection, you will have to request resources with Slurm . Once you decide on what resources you want, you will submit that information to the scheduler, which will place your job in the queue. When the resources are available, your job will start. Requesting resources in the cluster and all parameters allowed is discussed in more detail here . Starting Job \u00b6 Once the scheduler has started your job, it will run on some node in the cluster, using some resources that were defined by your parameters. It is not important what node the job runs on from the point of view of the client. Ending Job \u00b6 Once the job has finished, the scheduler will return whatever info you requested in your parameters. How the Unity Cluster Works \u00b6 It's recommended that you read the \"HPC Jargon\" section of the common terms page. The Basics \u00b6 Unity is a High Performance Computing Cluster (HPC Cluster) . That is, a system made up of many smaller computers ( nodes ) that work together to behave like one large computer. An HPC cluster focuses most on computational power and efficiency, as the name entails. HPC allows a user to utilize the power of many computers simultaneously. This is useful for large tasks that are infeasible for a common laptop or desktop. When you use your personal computer, the operating system decides what resources (CPU, RAM, etc.) should be used for a given task. For example, if you were to open a calculator program and multiply some numbers, the operating system would determine what CPU threads should be used to perform the calculation, and how much of the graphics card should be used in rendering the calculator on your screen. An HPC job scheduler does the same thing but on a much bigger scale. Instead of a single process on one machine, an HPC cluster handles jobs which can be run on many machines at one time. With multiple users, the scheduler must manage incoming requests, reserve the resources, and connect those resources to those who have requested them. The Unity job scheduler is called Slurm . How Unity Works \u00b6 Here is a general step by step process that governs how the Unity Cluster works: Client connects to Unity using SSH or JupyterHub . Client schedules a job through Slurm. Slurm allocates resources and connects the client to them. Job runs. Interactive job: Client does their work until they close the session or run out of time. Batch job: The job runs until it finishes its tasks or runs out of time. Job returns output. Batch job: The output to the command line is saved by slurm to a file like slurm-<JobID>.out .","title":"Home"},{"location":"index.html#introduction-to-unity","text":"Welcome to Unity! The Unity cluster is a collaborative, multi-institutional high-performance computing cluster located at the Massachusetts Green High Performance Computing Center (MGHPCC) . The cluster is under active development and supports primarily research activities. Partnering institutions currently include UMass Amherst, UMass Dartmouth, and University of Rhode Island.","title":"Introduction to Unity"},{"location":"index.html#accessing-unity","text":"Faculty/PI's: If you are a Faculty member or Principal Investigator (PI) , please create an account on the Unity main page . Use your campus NetID and password to sign up for a Unity account. Once your account is created, go to Account Settings and request PI status by clicking the \"Request PI account\" button. Once your account is approved you will be able to approve adding additional users to your PI group in the \u201cMy Users\u201d button on the side menu. Adding users will grant them access to your PI /work directory. Users in your group need to request access to join your PI group. You cannot create accounts for your users. Students: In order to use Unity you must be working with a PI and they must grant you access to their PI group. Create an account on the Unity main page . Click \"Login/Request Account\". Click on your institution or search for it via the search bar. Once your account is created you will need to assign a PI to it. To do this, go to \"My PIs\" using the left side menu and click the large \"+\" button. Follow the prompts to generate your unique SSH key. You can search for your PI by name. Once found, click on \u201cSend Request\u201d. After your PI approves your request, you will have access to Unity and your PI\u2019s project code and data stored in Unity. You can view, join, or leave a PI group by clicking \"My PIs\". Note: If you are a student, do not request a PI account . This will slow the process down. You should assign a PI to your account by clicking the \"+\" button. Your PI will then be able to approve your account. Something not working for you? Send an email to hpc@umass.edu with as much detail as you can provide to open a support ticket. Need additional help? We offer office hours every week on Tuesdays 2:30-4 PM on Zoom . Be sure to check the cluster notes page for up-to-date information on any canceled/delayed office hours. Need expert help using the cluster and optimizing your code? We encourage you to schedule an appointment with one of our experienced HPC facilitators. Send an email to hpc@umass.edu and request an HPC facilitator consultation.","title":"Accessing Unity"},{"location":"index.html#connecting-to-the-cluster","text":"You can connect to Unity in two ways, an SSH connection (the standard Linux console), or an instance of JupyterHub: JupyterHub is the easiest to get up and going. To work with JupyterHub, it's a good idea to get aquainted with roughly how demanding the job you're uploading is though. You just need to be familiar with how roughly how many of each resource you will need (Image below). Most of the time you will only ever need a single CPU or GPU, but if you have terabytes of data to analyze, then you should probably consider getting multiple GPUs and CPUs. When connecting the portal, click on JupyterHub tab located at the bottom of the options list on the left side of the window. This will take you to the JupyterHub for Unity, which looks like this: You will be asked to select what computer resources you want/need for the job you want to upload. Once you attempt to spawn your notebook and resources become available, you will be able to use JupyterHub as if it is running on your own computer. SSH is the more traditional method of using an HPC cluster. You will connect to the login node of unity, and you will be responsible for starting your own jobs. This can be more useful than JupyterHubs for jobs that last a long time and must be left unattended, or to have much more refined control over the resources allocated for your job.","title":"Connecting to the cluster"},{"location":"index.html#requesting-resources","text":"If you are on an SSH connection, you will have to request resources with Slurm . Once you decide on what resources you want, you will submit that information to the scheduler, which will place your job in the queue. When the resources are available, your job will start. Requesting resources in the cluster and all parameters allowed is discussed in more detail here .","title":"Requesting Resources"},{"location":"index.html#starting-job","text":"Once the scheduler has started your job, it will run on some node in the cluster, using some resources that were defined by your parameters. It is not important what node the job runs on from the point of view of the client.","title":"Starting Job"},{"location":"index.html#ending-job","text":"Once the job has finished, the scheduler will return whatever info you requested in your parameters.","title":"Ending Job"},{"location":"index.html#how-the-unity-cluster-works","text":"It's recommended that you read the \"HPC Jargon\" section of the common terms page.","title":"How the Unity Cluster Works"},{"location":"index.html#the-basics","text":"Unity is a High Performance Computing Cluster (HPC Cluster) . That is, a system made up of many smaller computers ( nodes ) that work together to behave like one large computer. An HPC cluster focuses most on computational power and efficiency, as the name entails. HPC allows a user to utilize the power of many computers simultaneously. This is useful for large tasks that are infeasible for a common laptop or desktop. When you use your personal computer, the operating system decides what resources (CPU, RAM, etc.) should be used for a given task. For example, if you were to open a calculator program and multiply some numbers, the operating system would determine what CPU threads should be used to perform the calculation, and how much of the graphics card should be used in rendering the calculator on your screen. An HPC job scheduler does the same thing but on a much bigger scale. Instead of a single process on one machine, an HPC cluster handles jobs which can be run on many machines at one time. With multiple users, the scheduler must manage incoming requests, reserve the resources, and connect those resources to those who have requested them. The Unity job scheduler is called Slurm .","title":"The Basics"},{"location":"index.html#how-unity-works","text":"Here is a general step by step process that governs how the Unity Cluster works: Client connects to Unity using SSH or JupyterHub . Client schedules a job through Slurm. Slurm allocates resources and connects the client to them. Job runs. Interactive job: Client does their work until they close the session or run out of time. Batch job: The job runs until it finishes its tasks or runs out of time. Job returns output. Batch job: The output to the command line is saved by slurm to a file like slurm-<JobID>.out .","title":"How Unity Works"},{"location":"faq.html","text":"Frequently Asked Questions \u00b6 How do I connect to, and start using the cluster? \u00b6 Refer to connection instructions on connecting here . You can connect with Putty, SSH in your terminal, or JupyterHub in your browser. When I connect over SSH I get a message saying permission denied (public key) \u00b6 This can be due one of these common reasons: You have not provided your private key while connecting. ssh -i <private_key_location> <user>@unity.rc.umass.edu You are not assigned to at least one PI group. We require at least one PI to endorse your account before you can use the cluster. Request to join a PI on the My PIs page. You have not added a public key to your account on Unity yet. You can do this on the Account Settings page . Your login shell is invalid. In Account Settings , try \"/bin/bash\" or \"/bin/zsh\". You are a PI, and you are trying to use your PI group name to log in. Your login username should not start with pi_ . Where can I find software to use on the cluster? \u00b6 Most of our software is package installed and is available by default. Non standard and version specific software are available as modules. The command module av will print all available modules. module av <name> will filter the available modules. Then you can use module load <name> to load a module and have access to its binaries (executables). I'm looking for xyz software, could you install it? \u00b6 Most software that is requested is free for use. If this is the case we will install it for you, just send us an email at hpc@umass.edu titled \"software request: \\<name>\". If the software you want is licensed, we may be able to help since the campus often has site-wide licenses for many applications. Can I run containers on Unity? \u00b6 Yes! We support singularity containers, which are fully compatible with docker images. Run \"module load singularity\" to access it. How much storage do I get on Unity and is it backed up? \u00b6 Refer to storage information here . We do not provide backup solutions by default. We take snapshots of /home/ and /work/ every day at 1AM, but delete them after two days. When I try to queue a job I get denied for MaxCpuPerAccount. \u00b6 Resource limits are set per lab. Currently, they are 300 CPUs, and 64 GPUs. This allocation is shared across your entire PI group. I'm a PI and I would like to purchase hardware to buy-in to Unity. \u00b6 Great! Send us an email and we'll be happy to help. We are very flexible when it comes to the needs of research labs.","title":"FAQ"},{"location":"faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq.html#how-do-i-connect-to-and-start-using-the-cluster","text":"Refer to connection instructions on connecting here . You can connect with Putty, SSH in your terminal, or JupyterHub in your browser.","title":"How do I connect to, and start using the cluster?"},{"location":"faq.html#when-i-connect-over-ssh-i-get-a-message-saying-permission-denied-public-key","text":"This can be due one of these common reasons: You have not provided your private key while connecting. ssh -i <private_key_location> <user>@unity.rc.umass.edu You are not assigned to at least one PI group. We require at least one PI to endorse your account before you can use the cluster. Request to join a PI on the My PIs page. You have not added a public key to your account on Unity yet. You can do this on the Account Settings page . Your login shell is invalid. In Account Settings , try \"/bin/bash\" or \"/bin/zsh\". You are a PI, and you are trying to use your PI group name to log in. Your login username should not start with pi_ .","title":"When I connect over SSH I get a message saying permission denied (public key)"},{"location":"faq.html#where-can-i-find-software-to-use-on-the-cluster","text":"Most of our software is package installed and is available by default. Non standard and version specific software are available as modules. The command module av will print all available modules. module av <name> will filter the available modules. Then you can use module load <name> to load a module and have access to its binaries (executables).","title":"Where can I find software to use on the cluster?"},{"location":"faq.html#im-looking-for-xyz-software-could-you-install-it","text":"Most software that is requested is free for use. If this is the case we will install it for you, just send us an email at hpc@umass.edu titled \"software request: \\<name>\". If the software you want is licensed, we may be able to help since the campus often has site-wide licenses for many applications.","title":"I'm looking for xyz software, could you install it?"},{"location":"faq.html#can-i-run-containers-on-unity","text":"Yes! We support singularity containers, which are fully compatible with docker images. Run \"module load singularity\" to access it.","title":"Can I run containers on Unity?"},{"location":"faq.html#how-much-storage-do-i-get-on-unity-and-is-it-backed-up","text":"Refer to storage information here . We do not provide backup solutions by default. We take snapshots of /home/ and /work/ every day at 1AM, but delete them after two days.","title":"How much storage do I get on Unity and is it backed up?"},{"location":"faq.html#when-i-try-to-queue-a-job-i-get-denied-for-maxcpuperaccount","text":"Resource limits are set per lab. Currently, they are 300 CPUs, and 64 GPUs. This allocation is shared across your entire PI group.","title":"When I try to queue a job I get denied for MaxCpuPerAccount."},{"location":"faq.html#im-a-pi-and-i-would-like-to-purchase-hardware-to-buy-in-to-unity","text":"Great! Send us an email and we'll be happy to help. We are very flexible when it comes to the needs of research labs.","title":"I'm a PI and I would like to purchase hardware to buy-in to Unity."},{"location":"jargon.html","text":"Common Terms \u00b6 Here are some definitions for common terms used in this documentation: HPC Jargon \u00b6 High Performance Computer Cluster (HPC Cluster) : A system made up of many smaller computers that behaves like one large computer. HPC allows a user to utilize the power of many computers simultaneously. Scheduler : Distributer of resources. If you request 2 CPU cores for your job , the scheduler will reserve those cores and connect you to them. In a multi-threaded computer, the operating system also distributes resources and manages tasks like a job scheduler. The Scheduler for the Unity cluster is called Slurm . Node : One of the computers that serves as part of an HPC cluster . See node list . Login Node : This is what you are running on when you first log in to Unity via ssh . Only small tasks should be run here. The login nodes have strict CPU and memory limits to disincentivize running big tasks. Also known as a Head Node in other clusters. Compute Node : These are nodes intended for your big tasks. These are accessed by scheduling jobs in Slurm . Job : The task that you provide for the cluster to execute, and what resources should be allocated to do so. Partition : A grouping of nodes. See partition list . Queue : Often used interchangably with partition . Principal Investigator (PI) : The individual responsible for a research grant. All users on Unity must be tied to a PI. Learn more about PI's Software Jargon \u00b6 Package : Installed software. Includes binaries, libraries, and other files. Typically cannot be moved about the filesystem once installed. Binary : An executable file. Library : Allows distribution of C code about the filesystem. Learn more about libraries Module : Script that modifies your environment to include a package. See environment modules . Environment : Set of environment variables. Defines the state of your current login shell . See environment modules . Conda environment : Set of installed packages. Activating a conda environment modifies your shell environment to include these packages. See conda . Compile : A compiler takes source code and turns it into binaries and libraries. Source code alone cannot be run by an operating system, it must be compiled into machine code or interpreted by an interpreter. C is compiled, Python is interpreted (sort of). Learn more about compilers Compiled vs Interpreted Languages Linux Jargon \u00b6 Interface : The connection between you and your program. Graphical User Interface (GUI) : The general term for graphical applications, where you click buttons, type in text boxes, and see visuals. Anything that uses the mouse. Command Line Interface (CLI) : The general term for text-based applications, where you enter commands and then read whatever comes out. Uses only the keyboard. Console : Often used interchangably with terminal . Terminal : The terminal is the graphical application that you type your commands in. In windows, this can be the the Command Prompt ( cmd ) or the Windows Terminal App (recommended). In Mac, it's just called Terminal. The terminal runs the shell. You can think of the terminal as just the borders of the window where you type commands. You can think of the shell as what's inside those borders. Shell : The shell interprets your commands and starts processes on the operating system. It depends on the terminal to take input from the user and to display output to the user. The shell stores your environment and allows you to write shell scripts. Learn more about shells Linux Filesystem Jargon \u00b6 Symlink : Shortcut. Directory : Folder. Current Working Directory (CWD) : The directory that is currently open in your program (often the shell). In a path , the CWD can be referenced with the . symbol. Parent Directory : The parent is the directory that is one above this directory. Every directory has a parent, except the root . In a path , the parent directory can be referenced with the .. symbol. Path : A path is a list of directories delimited by / slashes, followed by the name of a file. If the filesystem were a city, a path would be the directions to your apartment. Each directory in the path would be which road to take at the next crossroads. Relative Path : A path that does not start from a known landmark but from your current location CWD . ./.conda/envs/testName/bin/activate is a relative path. Absolute Path : A path that starts from a known landmark root / . /home/$USER/.conda/envs/testName/bin/activate is an absolute path. Root : The filesystem can be visualized like a tree (not the plant, the data structure ). The root of the tree is the directory from which all others descend from.","title":"Common Terms"},{"location":"jargon.html#common-terms","text":"Here are some definitions for common terms used in this documentation:","title":"Common Terms"},{"location":"jargon.html#hpc-jargon","text":"High Performance Computer Cluster (HPC Cluster) : A system made up of many smaller computers that behaves like one large computer. HPC allows a user to utilize the power of many computers simultaneously. Scheduler : Distributer of resources. If you request 2 CPU cores for your job , the scheduler will reserve those cores and connect you to them. In a multi-threaded computer, the operating system also distributes resources and manages tasks like a job scheduler. The Scheduler for the Unity cluster is called Slurm . Node : One of the computers that serves as part of an HPC cluster . See node list . Login Node : This is what you are running on when you first log in to Unity via ssh . Only small tasks should be run here. The login nodes have strict CPU and memory limits to disincentivize running big tasks. Also known as a Head Node in other clusters. Compute Node : These are nodes intended for your big tasks. These are accessed by scheduling jobs in Slurm . Job : The task that you provide for the cluster to execute, and what resources should be allocated to do so. Partition : A grouping of nodes. See partition list . Queue : Often used interchangably with partition . Principal Investigator (PI) : The individual responsible for a research grant. All users on Unity must be tied to a PI. Learn more about PI's","title":"HPC Jargon"},{"location":"jargon.html#software-jargon","text":"Package : Installed software. Includes binaries, libraries, and other files. Typically cannot be moved about the filesystem once installed. Binary : An executable file. Library : Allows distribution of C code about the filesystem. Learn more about libraries Module : Script that modifies your environment to include a package. See environment modules . Environment : Set of environment variables. Defines the state of your current login shell . See environment modules . Conda environment : Set of installed packages. Activating a conda environment modifies your shell environment to include these packages. See conda . Compile : A compiler takes source code and turns it into binaries and libraries. Source code alone cannot be run by an operating system, it must be compiled into machine code or interpreted by an interpreter. C is compiled, Python is interpreted (sort of). Learn more about compilers Compiled vs Interpreted Languages","title":"Software Jargon"},{"location":"jargon.html#linux-jargon","text":"Interface : The connection between you and your program. Graphical User Interface (GUI) : The general term for graphical applications, where you click buttons, type in text boxes, and see visuals. Anything that uses the mouse. Command Line Interface (CLI) : The general term for text-based applications, where you enter commands and then read whatever comes out. Uses only the keyboard. Console : Often used interchangably with terminal . Terminal : The terminal is the graphical application that you type your commands in. In windows, this can be the the Command Prompt ( cmd ) or the Windows Terminal App (recommended). In Mac, it's just called Terminal. The terminal runs the shell. You can think of the terminal as just the borders of the window where you type commands. You can think of the shell as what's inside those borders. Shell : The shell interprets your commands and starts processes on the operating system. It depends on the terminal to take input from the user and to display output to the user. The shell stores your environment and allows you to write shell scripts. Learn more about shells","title":"Linux Jargon"},{"location":"jargon.html#linux-filesystem-jargon","text":"Symlink : Shortcut. Directory : Folder. Current Working Directory (CWD) : The directory that is currently open in your program (often the shell). In a path , the CWD can be referenced with the . symbol. Parent Directory : The parent is the directory that is one above this directory. Every directory has a parent, except the root . In a path , the parent directory can be referenced with the .. symbol. Path : A path is a list of directories delimited by / slashes, followed by the name of a file. If the filesystem were a city, a path would be the directions to your apartment. Each directory in the path would be which road to take at the next crossroads. Relative Path : A path that does not start from a known landmark but from your current location CWD . ./.conda/envs/testName/bin/activate is a relative path. Absolute Path : A path that starts from a known landmark root / . /home/$USER/.conda/envs/testName/bin/activate is an absolute path. Root : The filesystem can be visualized like a tree (not the plant, the data structure ). The root of the tree is the directory from which all others descend from.","title":"Linux Filesystem Jargon"},{"location":"roadmap.html","text":"Unity Cluster Roadmap \u00b6 Unity is in Beta! With the addition of additional service hardware and storage devices, we are more confident in the stability of the cluster, and thus are ready to call it in \"beta\" state. In beta phase expect planned downtime more frequently than production research clusters, but you can expect the downtime periods to be shorter and more informative. General \u00b6 Wider support for common applications Support for singularity containers within Slurm VNC sessions from within jupyterhub for running GUI applications Infiniband support for inter-node MPI Graphical usage report Support for multiple PIs attached to one user Support for dynamic creation of shared drives Website \u00b6 Finish writing user documenation. Add content to the landing page(s) Hardware \u00b6 Multiple service nodes for reliability High-speed flash storage node for scratch space 600+ TB spinning disk NAS for general storage 32 GPU nodes with 8 2080TIs each inside","title":"Unity Cluster Roadmap #"},{"location":"roadmap.html#unity-cluster-roadmap","text":"Unity is in Beta! With the addition of additional service hardware and storage devices, we are more confident in the stability of the cluster, and thus are ready to call it in \"beta\" state. In beta phase expect planned downtime more frequently than production research clusters, but you can expect the downtime periods to be shorter and more informative.","title":"Unity Cluster Roadmap"},{"location":"roadmap.html#general","text":"Wider support for common applications Support for singularity containers within Slurm VNC sessions from within jupyterhub for running GUI applications Infiniband support for inter-node MPI Graphical usage report Support for multiple PIs attached to one user Support for dynamic creation of shared drives","title":"General"},{"location":"roadmap.html#website","text":"Finish writing user documenation. Add content to the landing page(s)","title":"Website"},{"location":"roadmap.html#hardware","text":"Multiple service nodes for reliability High-speed flash storage node for scratch space 600+ TB spinning disk NAS for general storage 32 GPU nodes with 8 2080TIs each inside","title":"Hardware"},{"location":"MGHPCC/MGHPCC.html","text":"MGHPCC Transitions \u00b6 Latest updates on MGHPCC transitions available here. For Users Moving from Shared/MGHPCC to Unity The Unity cluster uses Slurm, an open-source batch system. Slurm offers the same basic functionalities as LSF: you can use it to submit jobs, monitor their progress, and kill them if necessary. Similar to LSF, a job can be a single command, a parallel program using MPI or openMP, or a complex script. Slurm also supports GPUs and advanced features like job arrays. Slurm works in the same way as LSF in that: you only need to specify the resources needed by your job, such as number of cores and GPUs (if applicable), memory, run-time, etc. Slurm will analyse your job's requirements and will automatically send it to the right partition. (Slurm uses partitions instead of queues, but the idea is the same.) The LSF commands that you use today to submit and monitor jobs \u2014 bsub, bjobs \u2014 and their various options will need to be replaced with their Slurm equivalent, which can be found here . A full list of Slurm commands and their equivalents can be found here . How to Transfer Files From One HPC to Another File transfer solutions can be found in our documentation here .","title":"MGHPCC Transitions"},{"location":"MGHPCC/MGHPCC.html#mghpcc-transitions","text":"Latest updates on MGHPCC transitions available here. For Users Moving from Shared/MGHPCC to Unity The Unity cluster uses Slurm, an open-source batch system. Slurm offers the same basic functionalities as LSF: you can use it to submit jobs, monitor their progress, and kill them if necessary. Similar to LSF, a job can be a single command, a parallel program using MPI or openMP, or a complex script. Slurm also supports GPUs and advanced features like job arrays. Slurm works in the same way as LSF in that: you only need to specify the resources needed by your job, such as number of cores and GPUs (if applicable), memory, run-time, etc. Slurm will analyse your job's requirements and will automatically send it to the right partition. (Slurm uses partitions instead of queues, but the idea is the same.) The LSF commands that you use today to submit and monitor jobs \u2014 bsub, bjobs \u2014 and their various options will need to be replaced with their Slurm equivalent, which can be found here . A full list of Slurm commands and their equivalents can be found here . How to Transfer Files From One HPC to Another File transfer solutions can be found in our documentation here .","title":"MGHPCC Transitions"},{"location":"buy-in/requirements.html","text":"Hardware Requirements \u00b6 Note This page is here for reference. Please do not purchase hardware intended for the Unity cluster until checking with the Unity team. General Node Requirements \u00b6 Each server purchased is required to have the following hard requirements: IPMI 2.0 supported BMC with remote KVM built-in with discrete RJ45 management port Physical Ports VGA Port USB Port 10G SFP+ Port or 25G SFP28 port Note: If you require higher bandwidth for your nodes, that can be discussed, but will most likely require purchasing an addition switch (see below) At least 250GB boot SSD (Solid-state drive) 2x redundant PSU (Power supply unit) Each server purchased is recommended to have the following soft requirements: 2x boot drive for redundancy Storage Node Requirements \u00b6 The storage controller should be an HBA card or a RAID card that can operate in HBA mode, since we will be using ZFS software raid. At least 2 data plane (10/25G) ports for redundant connection. 2x boot drives for redundancy Network Equipment \u00b6 On a case-by-case basis, it may be necessary to purchase network switches if enough hardware was purchased to span a rack. If that is the case, each rack requires the following for the top-of-rack data switch: For 10/25G Nodes Mellanox SN2410 For 100G Nodes Mellanox SN2700 In addition, a 1G switch is required for IPMI connection. For IPMI Any 1G switch that has L2 capability (Mellanox preferred)","title":"Hardware Requirements"},{"location":"buy-in/requirements.html#hardware-requirements","text":"Note This page is here for reference. Please do not purchase hardware intended for the Unity cluster until checking with the Unity team.","title":"Hardware Requirements"},{"location":"buy-in/requirements.html#general-node-requirements","text":"Each server purchased is required to have the following hard requirements: IPMI 2.0 supported BMC with remote KVM built-in with discrete RJ45 management port Physical Ports VGA Port USB Port 10G SFP+ Port or 25G SFP28 port Note: If you require higher bandwidth for your nodes, that can be discussed, but will most likely require purchasing an addition switch (see below) At least 250GB boot SSD (Solid-state drive) 2x redundant PSU (Power supply unit) Each server purchased is recommended to have the following soft requirements: 2x boot drive for redundancy","title":"General Node Requirements"},{"location":"buy-in/requirements.html#storage-node-requirements","text":"The storage controller should be an HBA card or a RAID card that can operate in HBA mode, since we will be using ZFS software raid. At least 2 data plane (10/25G) ports for redundant connection. 2x boot drives for redundancy","title":"Storage Node Requirements"},{"location":"buy-in/requirements.html#network-equipment","text":"On a case-by-case basis, it may be necessary to purchase network switches if enough hardware was purchased to span a rack. If that is the case, each rack requires the following for the top-of-rack data switch: For 10/25G Nodes Mellanox SN2410 For 100G Nodes Mellanox SN2700 In addition, a 1G switch is required for IPMI connection. For IPMI Any 1G switch that has L2 capability (Mellanox preferred)","title":"Network Equipment"},{"location":"buy-in/types.html","text":"Types of Priority \u00b6 Nodes you purchase will be included in the Unity ecosystem - meaning you'll be able to use the web portal, access all the same storage, and access JupyterLab. When purchasing hardware for integration with Unity, there are three types of \"priority\" that you can request for your hardware. Note The below only applies to compute hardware, storage hardware will almost always be owned by the purchasing PI only and not shared with users. Preemption (Strictest) \u00b6 Priority nodes are added to: preempt partitions (cpu-preempt/gpu-preempt), and a newly created lab partition for use only by your lab Preemption is enabled in this mode. This means that if a priority user (member of the lab) wants to start a job but the nodes are full, non-priority jobs will be requeued on the spot for priority jobs. The requeued job will start again once resources are available. Your nodes are added to cpu/gpu-preempt partition depending on the type, in addition to the priority partition. This results in immediate access to the full capabilities of your own hardware, but the strictest limitations for general users of the cluster. Queueing Priority \u00b6 Priority nodes are added to: general partitions (cpu/gpu), and a newly created lab partition for use only by your lab Preemption is disabled in this mode, but your lab still gets queueing priority, which means if jobs are waiting to start on your hardware, your lab's jobs will always start first and take precedent. However, if your nodes are already full of general jobs, you will not be able to access this space until those jobs are done. As a result, we generally restrict general jobs to short queue timing (24 hours) such that priority users will not need to wait over a week etc. for non-priority jobs. No Priority \u00b6 In this mode no priority partition is created, and your nodes are contributed directly to the general partitions. It is not clear how this will affect billing for general hardware yet.","title":"Types of Priority"},{"location":"buy-in/types.html#types-of-priority","text":"Nodes you purchase will be included in the Unity ecosystem - meaning you'll be able to use the web portal, access all the same storage, and access JupyterLab. When purchasing hardware for integration with Unity, there are three types of \"priority\" that you can request for your hardware. Note The below only applies to compute hardware, storage hardware will almost always be owned by the purchasing PI only and not shared with users.","title":"Types of Priority"},{"location":"buy-in/types.html#preemption-strictest","text":"Priority nodes are added to: preempt partitions (cpu-preempt/gpu-preempt), and a newly created lab partition for use only by your lab Preemption is enabled in this mode. This means that if a priority user (member of the lab) wants to start a job but the nodes are full, non-priority jobs will be requeued on the spot for priority jobs. The requeued job will start again once resources are available. Your nodes are added to cpu/gpu-preempt partition depending on the type, in addition to the priority partition. This results in immediate access to the full capabilities of your own hardware, but the strictest limitations for general users of the cluster.","title":"Preemption (Strictest)"},{"location":"buy-in/types.html#queueing-priority","text":"Priority nodes are added to: general partitions (cpu/gpu), and a newly created lab partition for use only by your lab Preemption is disabled in this mode, but your lab still gets queueing priority, which means if jobs are waiting to start on your hardware, your lab's jobs will always start first and take precedent. However, if your nodes are already full of general jobs, you will not be able to access this space until those jobs are done. As a result, we generally restrict general jobs to short queue timing (24 hours) such that priority users will not need to wait over a week etc. for non-priority jobs.","title":"Queueing Priority"},{"location":"buy-in/types.html#no-priority","text":"In this mode no priority partition is created, and your nodes are contributed directly to the general partitions. It is not clear how this will affect billing for general hardware yet.","title":"No Priority"},{"location":"connecting/jupyter.html","text":"JupyterHub Portal \u00b6 The simplest method of using the cluster is using the Jupyter portal. You can access the jupyter portal from the Home Page > \"JupyterLab\". Simply select the resources you want and get started right away. Comprehensive documentation for JupyterLab can be found here .","title":"Web - JupyterHub"},{"location":"connecting/jupyter.html#jupyterhub-portal","text":"The simplest method of using the cluster is using the Jupyter portal. You can access the jupyter portal from the Home Page > \"JupyterLab\". Simply select the resources you want and get started right away. Comprehensive documentation for JupyterLab can be found here .","title":"JupyterHub Portal"},{"location":"connecting/ssh.html","text":"SSH Connection \u00b6 The most traditional method of connecting to Unity is using an SSH connection. A shell is what you type commands into. The most common shell in linux is bash, which is what you will likely be using on Unity. SSH stands for \"secure shell\". Configure SSH Keys \u00b6 The authentication method we use for SSH connections is with public/private RSA keys. You can read more about the public/private key exchange here . For the purposes of this guide, you should know that there is a public key which is stored on the server, and a private key, which you keep on your local computer. Think of them like your name and your social security number, respectively. In very basic terms, you authenticate the public key with your private key and that allows you to login to Unity. You must save your public key on Unity by adding it in your account settings . If you are unsure how to generate a public/private key pair, simply click on 'Generate Key'. The public key will be added to our database, and the private key will be downloaded to your computer. Note It's recommended that you place this downloaded private key in your home directory's .ssh folder. This is C:/Users/YOUR_NAME/.ssh in Windows, /home/YOUR_NAME/.ssh in Linux, and /Users/YOUR_NAME in Mac . In the terminal, a shortcut for this directory is the ~ symbol. This command will make the move on any operating system: mv ~/Downloads/privkey.key ~/.ssh/unity-privkey.key On Linux/Mac, you will need to change the permissions on the file due to its importance to security. chmod 600 ~/.ssh/unity-privkey.key It's recommended that you also add a password to this file using the following command: ssh-keygen -p -f ~/.ssh/unity-privkey.rsa Connection Details \u00b6 If you know what to do with this information already, you can skip the rest of this guide. Hostname/Address: unity.rc.umass.edu Username: NETID_school_edu Note Your username is in the format <organization username>_<organization>_edu . View your username here CLI Users \u00b6 We recommend connecting to Unity via the terminal. Windows, Mac, and most distributions of linux come with the OpenSSH client, which you can use to connect to Unity in your terminal. If the file ~/.ssh/config doesn't exist, create it. Copy the following contents to your Notepad and replace <NETID> and <PATH_TO_PRIVATE_KEY> to your specifications: Remember to save the file in a directory of your choosing, without an extension. Host unity HostName unity.rc.umass.edu User <USERID>_<ORGANIZATION>_edu IdentityFile <PATH_TO_PRIVATE_KEY> Note Doing this with a text editor and a file explorer can be challenging because these user friendly methods don't like files without an extension, and the ssh config file must not have an extension. In Windows Notepad, you can save a file with no extension in the 'All Files' category, and windows will add the .txt extension regardless, which won't work. The Mac TextEdit doesn't even have the option to save as .txt , which is tremendously unhelpful. You can make your current file plain-text formatted using \u2318-\u21e7-T, and you can add plain-text as a 'Save as' option in the config. The most reliable way to put your OpenSSH config file in the correct location is to open the terminal and use the mv (move) command, which will rename files with no fuss. mv path/to/source-file path/to/desination-file mv ~/Desktop/ssh-config.txt ~/.ssh/config Once the OpenSSH config file is in place, you can connect to Unity in your terminal using the command ssh unity . Windows GUI Users \u00b6 Windows users can use PuTTY to connect to Unity. Download and install PuTTY by following the link above. Be sure to select the 64 bit / 32 bit download depending on your system. Most are 64 bit, but if you are unsure 32 bit will always work. Open PuTTY and enter hostname unity.rc.umass.edu on the main page On the left sidebar, navigate to connection -> data, and enter your username. On the left sidebar, navigate to connction -> ssh -> auth, and browse to your private key location. Finally, in the main screen again, save the profile you created so you don't have to enter this information every time. Enter unity as the profile name and click save . You can then double click on unity under saved sessions to connect to Unity right away. The server's ssh-ed25519 key fingerprint is: ssh-ed25519 255 SHA256:jC7BF7h5/RJo5Svx1v+lufdf+I/ogu5dQV2sUe+y8ek If this key matches what is on your terminal, go ahead and click \"Accept\".","title":"Console - SSH"},{"location":"connecting/ssh.html#ssh-connection","text":"The most traditional method of connecting to Unity is using an SSH connection. A shell is what you type commands into. The most common shell in linux is bash, which is what you will likely be using on Unity. SSH stands for \"secure shell\".","title":"SSH Connection"},{"location":"connecting/ssh.html#configure-ssh-keys","text":"The authentication method we use for SSH connections is with public/private RSA keys. You can read more about the public/private key exchange here . For the purposes of this guide, you should know that there is a public key which is stored on the server, and a private key, which you keep on your local computer. Think of them like your name and your social security number, respectively. In very basic terms, you authenticate the public key with your private key and that allows you to login to Unity. You must save your public key on Unity by adding it in your account settings . If you are unsure how to generate a public/private key pair, simply click on 'Generate Key'. The public key will be added to our database, and the private key will be downloaded to your computer. Note It's recommended that you place this downloaded private key in your home directory's .ssh folder. This is C:/Users/YOUR_NAME/.ssh in Windows, /home/YOUR_NAME/.ssh in Linux, and /Users/YOUR_NAME in Mac . In the terminal, a shortcut for this directory is the ~ symbol. This command will make the move on any operating system: mv ~/Downloads/privkey.key ~/.ssh/unity-privkey.key On Linux/Mac, you will need to change the permissions on the file due to its importance to security. chmod 600 ~/.ssh/unity-privkey.key It's recommended that you also add a password to this file using the following command: ssh-keygen -p -f ~/.ssh/unity-privkey.rsa","title":"Configure SSH Keys"},{"location":"connecting/ssh.html#connection-details","text":"If you know what to do with this information already, you can skip the rest of this guide. Hostname/Address: unity.rc.umass.edu Username: NETID_school_edu Note Your username is in the format <organization username>_<organization>_edu . View your username here","title":"Connection Details"},{"location":"connecting/ssh.html#cli-users","text":"We recommend connecting to Unity via the terminal. Windows, Mac, and most distributions of linux come with the OpenSSH client, which you can use to connect to Unity in your terminal. If the file ~/.ssh/config doesn't exist, create it. Copy the following contents to your Notepad and replace <NETID> and <PATH_TO_PRIVATE_KEY> to your specifications: Remember to save the file in a directory of your choosing, without an extension. Host unity HostName unity.rc.umass.edu User <USERID>_<ORGANIZATION>_edu IdentityFile <PATH_TO_PRIVATE_KEY> Note Doing this with a text editor and a file explorer can be challenging because these user friendly methods don't like files without an extension, and the ssh config file must not have an extension. In Windows Notepad, you can save a file with no extension in the 'All Files' category, and windows will add the .txt extension regardless, which won't work. The Mac TextEdit doesn't even have the option to save as .txt , which is tremendously unhelpful. You can make your current file plain-text formatted using \u2318-\u21e7-T, and you can add plain-text as a 'Save as' option in the config. The most reliable way to put your OpenSSH config file in the correct location is to open the terminal and use the mv (move) command, which will rename files with no fuss. mv path/to/source-file path/to/desination-file mv ~/Desktop/ssh-config.txt ~/.ssh/config Once the OpenSSH config file is in place, you can connect to Unity in your terminal using the command ssh unity .","title":"CLI Users"},{"location":"connecting/ssh.html#windows-gui-users","text":"Windows users can use PuTTY to connect to Unity. Download and install PuTTY by following the link above. Be sure to select the 64 bit / 32 bit download depending on your system. Most are 64 bit, but if you are unsure 32 bit will always work. Open PuTTY and enter hostname unity.rc.umass.edu on the main page On the left sidebar, navigate to connection -> data, and enter your username. On the left sidebar, navigate to connction -> ssh -> auth, and browse to your private key location. Finally, in the main screen again, save the profile you created so you don't have to enter this information every time. Enter unity as the profile name and click save . You can then double click on unity under saved sessions to connect to Unity right away. The server's ssh-ed25519 key fingerprint is: ssh-ed25519 255 SHA256:jC7BF7h5/RJo5Svx1v+lufdf+I/ogu5dQV2sUe+y8ek If this key matches what is on your terminal, go ahead and click \"Accept\".","title":"Windows GUI Users"},{"location":"slurm/index.html","text":"Introduction to Slurm: The Job Scheduler \u00b6 Slurm is the job scheduler we use in Unity. More info about what a job scheduler is can be found in the introduction . Here we will go more into depth about some elements of the scheduler. There are many more features of Slurm that go beyond the scope of this guide, but all that you as a user need to know should be available. Note Doing work on the login nodes can cause Unity to become sluggish for the entire user base. We have disincentivized this by setting limits on cpu and memory. Learn how to use srun interactive sessions to switch from a login node to a compute node. Core Limits \u00b6 There is currently a 300 CPU core, 64 GPU limit to be shared by the users of each lab. When you try to go over this limit, you will be denied for MaxCpuPerAccount . Partitions / Queues \u00b6 Our cluster has a number of slurm partitions defined, also known as a queue . As you may have guessed, you as the user request to use a specific partition based on what resources your job needs. Find out which partition is best for your job here . Jobs \u00b6 A job is an operation which the user submits to the cluster to run under allocated resources. There are two commands for this, srun and sbatch . srun is tied to your current session, and can allow you to interact with your job. sbatch is not tied to your current session, so you can start it and walk away. If you want to interact with your job and be able to walk away, you can use tmux to make a detachable session. (see below) SRUN \u00b6 An srun job is tied to your ssh session. If you break (ctrl+C) or close your ssh session during an srun job, the job will be killed . You can also make an interactive job, which will allow your job to take input from your keyboard. You can run bash in an interactive job to resume your work on a compute node just as you would on a login node. This is highly recommended. See SRUN Jobs for more information. SBATCH \u00b6 An sbatch job is submitted to the cluster with no information returned to the user other than a Job ID. An sbatch job will try to create a file in your current working directory that contains the results of your job. See SBATCH Jobs for more information. TMUX SRUN \u00b6 tmux # tmux session opens srun --pty -c 1 bash # interactive job on compute node opens with one cpu core sleep 3600; echo \"done\" # interactive job will have blinking cursor for an hour # > ctrl+b # tmux keyboard-shortcut command mode opens # > d # tmux session detaches, back to login node # at this point you can log off and log back in without killing the job tmux ls # print list of tmux sessions # first number on the left (call it X) is needed to re-attach the session tmux attach-session -t X # back to interactive job","title":"Introduction"},{"location":"slurm/index.html#introduction-to-slurm-the-job-scheduler","text":"Slurm is the job scheduler we use in Unity. More info about what a job scheduler is can be found in the introduction . Here we will go more into depth about some elements of the scheduler. There are many more features of Slurm that go beyond the scope of this guide, but all that you as a user need to know should be available. Note Doing work on the login nodes can cause Unity to become sluggish for the entire user base. We have disincentivized this by setting limits on cpu and memory. Learn how to use srun interactive sessions to switch from a login node to a compute node.","title":"Introduction to Slurm: The Job Scheduler"},{"location":"slurm/index.html#core-limits","text":"There is currently a 300 CPU core, 64 GPU limit to be shared by the users of each lab. When you try to go over this limit, you will be denied for MaxCpuPerAccount .","title":"Core Limits"},{"location":"slurm/index.html#partitions-queues","text":"Our cluster has a number of slurm partitions defined, also known as a queue . As you may have guessed, you as the user request to use a specific partition based on what resources your job needs. Find out which partition is best for your job here .","title":"Partitions / Queues"},{"location":"slurm/index.html#jobs","text":"A job is an operation which the user submits to the cluster to run under allocated resources. There are two commands for this, srun and sbatch . srun is tied to your current session, and can allow you to interact with your job. sbatch is not tied to your current session, so you can start it and walk away. If you want to interact with your job and be able to walk away, you can use tmux to make a detachable session. (see below)","title":"Jobs"},{"location":"slurm/index.html#srun","text":"An srun job is tied to your ssh session. If you break (ctrl+C) or close your ssh session during an srun job, the job will be killed . You can also make an interactive job, which will allow your job to take input from your keyboard. You can run bash in an interactive job to resume your work on a compute node just as you would on a login node. This is highly recommended. See SRUN Jobs for more information.","title":"SRUN"},{"location":"slurm/index.html#sbatch","text":"An sbatch job is submitted to the cluster with no information returned to the user other than a Job ID. An sbatch job will try to create a file in your current working directory that contains the results of your job. See SBATCH Jobs for more information.","title":"SBATCH"},{"location":"slurm/index.html#tmux-srun","text":"tmux # tmux session opens srun --pty -c 1 bash # interactive job on compute node opens with one cpu core sleep 3600; echo \"done\" # interactive job will have blinking cursor for an hour # > ctrl+b # tmux keyboard-shortcut command mode opens # > d # tmux session detaches, back to login node # at this point you can log off and log back in without killing the job tmux ls # print list of tmux sessions # first number on the left (call it X) is needed to re-attach the session tmux attach-session -t X # back to interactive job","title":"TMUX SRUN"},{"location":"slurm/mpi.html","text":"Running Jobs on Multple Nodes Using MPI \u00b6 This is the (simplified) sanity check that the Unity admins used to verify that MPI is working: \u00b6 Note We recommend when running quick jobs that you put your job in a preempt partition for a shorter wait time. srun -p cpu-preempt ... srun \u00b6 module load openmpi mpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array srun --pty -N 2 mpi_array sbatch \u00b6 module load openmpi mpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array sbatch -N 2 mpi_script Where mpi_script is a file containing the following: #!/bin/bash srun mpi_array Frequently Asked Questions \u00b6 Why can't I use mpirun / mpiexec ? \u00b6 This version of Spack (openmpi ~legacylaunchers schedulers=slurm) is installed without the mpiexec/mpirun commands to prevent unintended performance issues. See https://github.com/spack/spack/pull/10340 for more details. If you understand the potential consequences of a misconfigured mpirun, you can use spack to install 'openmpi+legacylaunchers' to restore the executables. Otherwise, use srun to launch your MPI executables. The community of HPC admins at Spack have agreed that using mpirun with slurm is a bad idea. srun is capable of doing all that mpirun is, and having the two fight over control is reported to cause poor performance. We currently have a module called openmpi+mpirun , which was installed in Spack with +legacylaunchers , enabling you to use the wrappers as you please. Why do I get multiple outputs from my mpi aware binaries? \u00b6 This is because message passing is not working. You are running duplicates of your job in parallel, which are unaware of each other. If using srun , you should make sure that you have the pseudo-terminal enabled --pty . This should not occur with sbatch . Why do I keep getting PMIX ERROR: NO-PERMISSIONS in file ? \u00b6 openmpi is dependent on pmix. Our current system slurm installation was not configured with pmix support. This is evident by srun --mpi=list . pmix_v2 and pmix_v3 should be there, but they aren't. We will likely recomile slurm to accomodate this. Why do I keep getting this OpenFabrics warning? \u00b6 \"No OpenFabrics connection schemes reported that they were able to be used on a specific port. As such, the openib BTL (OpenFabrics support) will be disabled for this port.\" We do not currently have infiniband hardware in our network, and openmpi would like us to. You can simply add -mca btl ^ofi to your mpirun command and disable the infiniband feature. We will likely recompile openmpi to disable this sitewide. mpirun -mca btl ^ofi ...","title":"MPI"},{"location":"slurm/mpi.html#running-jobs-on-multple-nodes-using-mpi","text":"","title":"Running Jobs on Multple Nodes Using MPI"},{"location":"slurm/mpi.html#this-is-the-simplified-sanity-check-that-the-unity-admins-used-to-verify-that-mpi-is-working","text":"Note We recommend when running quick jobs that you put your job in a preempt partition for a shorter wait time. srun -p cpu-preempt ...","title":"This is the (simplified) sanity check that the Unity admins used to verify that MPI is working:"},{"location":"slurm/mpi.html#srun","text":"module load openmpi mpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array srun --pty -N 2 mpi_array","title":"srun"},{"location":"slurm/mpi.html#sbatch","text":"module load openmpi mpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array sbatch -N 2 mpi_script Where mpi_script is a file containing the following: #!/bin/bash srun mpi_array","title":"sbatch"},{"location":"slurm/mpi.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"slurm/mpi.html#why-cant-i-use-mpirunmpiexec","text":"This version of Spack (openmpi ~legacylaunchers schedulers=slurm) is installed without the mpiexec/mpirun commands to prevent unintended performance issues. See https://github.com/spack/spack/pull/10340 for more details. If you understand the potential consequences of a misconfigured mpirun, you can use spack to install 'openmpi+legacylaunchers' to restore the executables. Otherwise, use srun to launch your MPI executables. The community of HPC admins at Spack have agreed that using mpirun with slurm is a bad idea. srun is capable of doing all that mpirun is, and having the two fight over control is reported to cause poor performance. We currently have a module called openmpi+mpirun , which was installed in Spack with +legacylaunchers , enabling you to use the wrappers as you please.","title":"Why can't I use mpirun/mpiexec?"},{"location":"slurm/mpi.html#why-do-i-get-multiple-outputs-from-my-mpi-aware-binaries","text":"This is because message passing is not working. You are running duplicates of your job in parallel, which are unaware of each other. If using srun , you should make sure that you have the pseudo-terminal enabled --pty . This should not occur with sbatch .","title":"Why do I get multiple outputs from my mpi aware binaries?"},{"location":"slurm/mpi.html#why-do-i-keep-getting-pmix-error-no-permissions-in-file","text":"openmpi is dependent on pmix. Our current system slurm installation was not configured with pmix support. This is evident by srun --mpi=list . pmix_v2 and pmix_v3 should be there, but they aren't. We will likely recomile slurm to accomodate this.","title":"Why do I keep getting PMIX ERROR: NO-PERMISSIONS in file?"},{"location":"slurm/mpi.html#why-do-i-keep-getting-this-openfabrics-warning","text":"\"No OpenFabrics connection schemes reported that they were able to be used on a specific port. As such, the openib BTL (OpenFabrics support) will be disabled for this port.\" We do not currently have infiniband hardware in our network, and openmpi would like us to. You can simply add -mca btl ^ofi to your mpirun command and disable the infiniband feature. We will likely recompile openmpi to disable this sitewide. mpirun -mca btl ^ofi ...","title":"Why do I keep getting this OpenFabrics warning?"},{"location":"slurm/sbatch.html","text":"Using SBATCH to Submit Jobs \u00b6 SBATCH is a non-blocking command, meaning there is not a circumstance where running the command will cause it to hold. Even if the resources requested are not available, the job will be thrown into the queue and will start to run once resources become available. The status of a job can be seen using squeue while it is pending or running and sacct at any time. squeue --me sacct -j YOUR_JOBID SBATCH is based around running a single file. That being said, you shouldn't need to specify any parameters in the command other than sbatch <batch file> , because you can specify all parameters in the command inside the file itself. The following is an example of a batch script. Please note that the top of the script must start with #!/bin/bash (or whatever interpreter you need, if you don't know, use bash), and then immediately follow with #SBATCH <param> parameters. An example of common SBATCH parameters and a simple script is below, this script will allocate 4 CPUs and one GPU in the GPU partition. #!/bin/bash #SBATCH -c 4 # Number of Cores per Task #SBATCH --mem=8192 # Requested Memory #SBATCH -p gpu # Partition #SBATCH -G 1 # Number of GPUs #SBATCH -t 01:00:00 # Job time limit #SBATCH -o slurm-%j.out # %j = job ID module load cuda/10 /modules/apps/cuda/10.1.243/samples/bin/x86_64/linux/release/deviceQuery This script should query the available GPUs, and print only one device to the specified file. Feel free to remove/modify any of the parameters in the script to suit your needs.","title":"SBATCH Jobs"},{"location":"slurm/sbatch.html#using-sbatch-to-submit-jobs","text":"SBATCH is a non-blocking command, meaning there is not a circumstance where running the command will cause it to hold. Even if the resources requested are not available, the job will be thrown into the queue and will start to run once resources become available. The status of a job can be seen using squeue while it is pending or running and sacct at any time. squeue --me sacct -j YOUR_JOBID SBATCH is based around running a single file. That being said, you shouldn't need to specify any parameters in the command other than sbatch <batch file> , because you can specify all parameters in the command inside the file itself. The following is an example of a batch script. Please note that the top of the script must start with #!/bin/bash (or whatever interpreter you need, if you don't know, use bash), and then immediately follow with #SBATCH <param> parameters. An example of common SBATCH parameters and a simple script is below, this script will allocate 4 CPUs and one GPU in the GPU partition. #!/bin/bash #SBATCH -c 4 # Number of Cores per Task #SBATCH --mem=8192 # Requested Memory #SBATCH -p gpu # Partition #SBATCH -G 1 # Number of GPUs #SBATCH -t 01:00:00 # Job time limit #SBATCH -o slurm-%j.out # %j = job ID module load cuda/10 /modules/apps/cuda/10.1.243/samples/bin/x86_64/linux/release/deviceQuery This script should query the available GPUs, and print only one device to the specified file. Feel free to remove/modify any of the parameters in the script to suit your needs.","title":"Using SBATCH to Submit Jobs"},{"location":"slurm/srun.html","text":"Using SRUN to Submit Jobs \u00b6 Note Usually, if you have to run a single application multiple times, or if you are trying to run a non-interactive application, you should use sbatch instead of srun, since sbatch allows you to specify parameters in the file, and is non-blocking (see below). SRUN is a so-called blocking command, as in it will not let you execute other commands until this command is finished (not necessarily the job, just the allocation). For example, if you run srun /bin/hostname and resources are available right away, the job will be sent out and the result saved into a file. If resources are not available, you will be stuck in the command while you are pending in the queue. Please note that like sbatch, you can run a batch file using srun. The command syntax is srun <options> [executable] <args> Options is where you can specify the resources you want for the executable, or define. The following are some of the options available; to see all available parameters run man srun . -c <num> Number of CPUs (threads) to allocate to the job per task -n <num> The number of tasks to allocate (for MPI) -G <num> Number of GPUs to allocate to the job --mem <num>[K|M|G|T] Memory to allocate to the job (in MB by default) -p <partition> Partition to submit the job to To run an interacitve job (in this case a bash prompt), the command might look like this ( --pty is the important option): srun -c 6 -p cpu --pty bash To run an application on the cluster that uses a GUI, you must use an interactive job, in addition to the --x11 argument: srun -c 6 -p cpu --pty --x11 xclock Note You cannot run an interactive/gui job using the sbatch command, you must use srun .","title":"SRUN Jobs"},{"location":"slurm/srun.html#using-srun-to-submit-jobs","text":"Note Usually, if you have to run a single application multiple times, or if you are trying to run a non-interactive application, you should use sbatch instead of srun, since sbatch allows you to specify parameters in the file, and is non-blocking (see below). SRUN is a so-called blocking command, as in it will not let you execute other commands until this command is finished (not necessarily the job, just the allocation). For example, if you run srun /bin/hostname and resources are available right away, the job will be sent out and the result saved into a file. If resources are not available, you will be stuck in the command while you are pending in the queue. Please note that like sbatch, you can run a batch file using srun. The command syntax is srun <options> [executable] <args> Options is where you can specify the resources you want for the executable, or define. The following are some of the options available; to see all available parameters run man srun . -c <num> Number of CPUs (threads) to allocate to the job per task -n <num> The number of tasks to allocate (for MPI) -G <num> Number of GPUs to allocate to the job --mem <num>[K|M|G|T] Memory to allocate to the job (in MB by default) -p <partition> Partition to submit the job to To run an interacitve job (in this case a bash prompt), the command might look like this ( --pty is the important option): srun -c 6 -p cpu --pty bash To run an application on the cluster that uses a GUI, you must use an interactive job, in addition to the --x11 argument: srun -c 6 -p cpu --pty --x11 xclock Note You cannot run an interactive/gui job using the sbatch command, you must use srun .","title":"Using SRUN to Submit Jobs"},{"location":"software/index.html","text":"Unity Software Overview \u00b6 Means of installing packages \u00b6 apt package manager \u00b6 The Ubuntu system package manager apt downloads its packages pre-compiled from the Ubuntu repository . These are placed in standard locations like /usr/bin so that they are always found in your $PATH . This can only be done by administrators. To avoid conflicts, most software is only available in one version. These packages will change with an (enevitable) operating system update . The admin team is trying to avoid apt installs for research computing software. Relevant Documentation: None Environment Modules \u00b6 There are a wide variety of modules available with the module command. Most software requests are fulfilled here. These are compiled by the admins and are stored in /modules/apps/ or in /modules/spack/opt/spack/ . Relevant Documentation: intro to environment modules using environment modules module hierarchy Conda \u00b6 The conda package manager allows users to compile software easily and without admin privileges. Conda environments can be created for any software set, and can be enabled/disabled dynamically not unlike modules. Relevant Documentation: conda R package manager \u00b6 Coming soon! Docker (Singularity) \u00b6 Coming soon!","title":"Overview"},{"location":"software/index.html#unity-software-overview","text":"","title":"Unity Software Overview"},{"location":"software/index.html#means-of-installing-packages","text":"","title":"Means of installing packages"},{"location":"software/index.html#apt-package-manager","text":"The Ubuntu system package manager apt downloads its packages pre-compiled from the Ubuntu repository . These are placed in standard locations like /usr/bin so that they are always found in your $PATH . This can only be done by administrators. To avoid conflicts, most software is only available in one version. These packages will change with an (enevitable) operating system update . The admin team is trying to avoid apt installs for research computing software. Relevant Documentation: None","title":"apt package manager"},{"location":"software/index.html#environment-modules","text":"There are a wide variety of modules available with the module command. Most software requests are fulfilled here. These are compiled by the admins and are stored in /modules/apps/ or in /modules/spack/opt/spack/ . Relevant Documentation: intro to environment modules using environment modules module hierarchy","title":"Environment Modules"},{"location":"software/index.html#conda","text":"The conda package manager allows users to compile software easily and without admin privileges. Conda environments can be created for any software set, and can be enabled/disabled dynamically not unlike modules. Relevant Documentation: conda","title":"Conda"},{"location":"software/index.html#r-package-manager","text":"Coming soon!","title":"R package manager"},{"location":"software/index.html#docker-singularity","text":"Coming soon!","title":"Docker (Singularity)"},{"location":"software/conda.html","text":"Using Conda Environments \u00b6 Introduction \u00b6 The conda package manager allows users to compile software easily and without admin privileges. Conda environments can be created for any software set, and can be enabled/disabled dynamically not unlike modules. A conda environment is not to be confused with the environment of your login shell . The package tied to an environment module is compiled by hand by the Unity admins, where conda packages can be installed by any user with a simple command . A conda environment can contain any number of packages, where a module usually only contains one. Modules and conda environments can be used together. On Unity we use Miniconda, as opposed to Anaconda. From a user's perspective they can be considered to be the same thing. Note When working on a conda environment, make sure you activate it! Without a currently active environment, conda will attempt to modify the global Unity environment, and you will get permission denied . Setup \u00b6 The conda command is not available unless the miniconda module is loaded. If you see this: conda: Command not found Do this: module load miniconda Creating an Environment \u00b6 You can create as many conda environments as you desire, limited only by our disk quotas . conda create --name testName python=3.7 This creates an environment in your home folder, specifically /home/$USER/.conda/envs/<name> . You can also create environments in other directories, such as your PI's work directory. mkdir -p /work/pi_name/$USER-conda/envs conda create --prefix /work/pi_name/$USER-conda/envs/testName python=3.7 # OPTIONAL, make a symlink (shortcut) to home directory ln -s /work/pi_name/$USER-conda/envs/testName ~/testName Replace testName with the name of your choice, and replace 3.7 with your Python version of choice. Note The $USER environment variable evaluates to your username. Activating an Environment \u00b6 Environment created with --name : conda activate testName Environment created with --prefix : conda activate /work/pi_name/$USER-conda/envs/testName # OR cd /work/pi_name/$USER-conda/envs/ conda activate ./testName Your currently active conda environment will appear in parentheses to the left of your command line prompt: user@login2:~$ conda activate ./testName (testName) user@login2:~$ Adding Packages to your Environment \u00b6 conda install numpy The install will ask you to confirm installing numpy as well as any other additional required packages. List Available Environments \u00b6 conda env list List Packages Installed in the Current Environment \u00b6 conda list Delete an Environment \u00b6 conda remove --name testName --all If your environment was added to JupyterHub, you will have to remove it manually. rm -rf ~/.local/share/jupyter/kernels/testName Conda Environments and Jupyter \u00b6 You can create many custom conda environments and use them within JupyterHub. This must be done in the command line, but JupyterHub provides a command line interface in it's 'Terminal' app. Adding your Environment to JupyterHub \u00b6 Note make sure your environment is activated first. Without a currently active environment, conda will attempt to modify the main default environment, and you will get permission denied. conda install ipykernel Add a kernelspec (Kernel Specification) to your JupyterHub. python -m ipykernel install --user --name testName --display-name=\"Display Name Within JupyterHub\" If the above was done within JupyterHub, reload the page. If that doesn't work, restart your JupyterHub server. Learn more \u00b6 Conda documentation","title":"Conda"},{"location":"software/conda.html#using-conda-environments","text":"","title":"Using Conda Environments"},{"location":"software/conda.html#introduction","text":"The conda package manager allows users to compile software easily and without admin privileges. Conda environments can be created for any software set, and can be enabled/disabled dynamically not unlike modules. A conda environment is not to be confused with the environment of your login shell . The package tied to an environment module is compiled by hand by the Unity admins, where conda packages can be installed by any user with a simple command . A conda environment can contain any number of packages, where a module usually only contains one. Modules and conda environments can be used together. On Unity we use Miniconda, as opposed to Anaconda. From a user's perspective they can be considered to be the same thing. Note When working on a conda environment, make sure you activate it! Without a currently active environment, conda will attempt to modify the global Unity environment, and you will get permission denied .","title":"Introduction"},{"location":"software/conda.html#setup","text":"The conda command is not available unless the miniconda module is loaded. If you see this: conda: Command not found Do this: module load miniconda","title":"Setup"},{"location":"software/conda.html#creating-an-environment","text":"You can create as many conda environments as you desire, limited only by our disk quotas . conda create --name testName python=3.7 This creates an environment in your home folder, specifically /home/$USER/.conda/envs/<name> . You can also create environments in other directories, such as your PI's work directory. mkdir -p /work/pi_name/$USER-conda/envs conda create --prefix /work/pi_name/$USER-conda/envs/testName python=3.7 # OPTIONAL, make a symlink (shortcut) to home directory ln -s /work/pi_name/$USER-conda/envs/testName ~/testName Replace testName with the name of your choice, and replace 3.7 with your Python version of choice. Note The $USER environment variable evaluates to your username.","title":"Creating an Environment"},{"location":"software/conda.html#activating-an-environment","text":"Environment created with --name : conda activate testName Environment created with --prefix : conda activate /work/pi_name/$USER-conda/envs/testName # OR cd /work/pi_name/$USER-conda/envs/ conda activate ./testName Your currently active conda environment will appear in parentheses to the left of your command line prompt: user@login2:~$ conda activate ./testName (testName) user@login2:~$","title":"Activating an Environment"},{"location":"software/conda.html#adding-packages-to-your-environment","text":"conda install numpy The install will ask you to confirm installing numpy as well as any other additional required packages.","title":"Adding Packages to your Environment"},{"location":"software/conda.html#list-available-environments","text":"conda env list","title":"List Available Environments"},{"location":"software/conda.html#list-packages-installed-in-the-current-environment","text":"conda list","title":"List Packages Installed in the Current Environment"},{"location":"software/conda.html#delete-an-environment","text":"conda remove --name testName --all If your environment was added to JupyterHub, you will have to remove it manually. rm -rf ~/.local/share/jupyter/kernels/testName","title":"Delete an Environment"},{"location":"software/conda.html#conda-environments-and-jupyter","text":"You can create many custom conda environments and use them within JupyterHub. This must be done in the command line, but JupyterHub provides a command line interface in it's 'Terminal' app.","title":"Conda Environments and Jupyter"},{"location":"software/conda.html#adding-your-environment-to-jupyterhub","text":"Note make sure your environment is activated first. Without a currently active environment, conda will attempt to modify the main default environment, and you will get permission denied. conda install ipykernel Add a kernelspec (Kernel Specification) to your JupyterHub. python -m ipykernel install --user --name testName --display-name=\"Display Name Within JupyterHub\" If the above was done within JupyterHub, reload the page. If that doesn't work, restart your JupyterHub server.","title":"Adding your Environment to JupyterHub"},{"location":"software/conda.html#learn-more","text":"Conda documentation","title":"Learn more"},{"location":"software/module-hierarchy.html","text":"Module Hierarchy \u00b6 Environment Modules is a tool to change dynamically what software is available for use by a given user at a given time. Before you read this, it's recommended that you first read the introduction and the module usage guide . As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. Our strategy is to create a module hierarchy to divide modules by their stacks. The modulepath environment variable $MODULEPATH is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default. This means not all modules can be found with module avail by default. Here is the full Unity module hierarchy as of 2022/10/26: \u00b6 Compilers are red, providers are blue, and default directories are bold. /modules/modulefiles/ x86_64 |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-h3cppyo/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-6pbqv7b/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-3rgk3nu/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-gmusbfh/ | | \\\u2500 gcc /9.4.0/ cascadelake |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-ad5zrqt/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-cuu4pwk/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-lih7mwq/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-w2r5zyv/ | | \\\u2500 gcc /9.4.0/ haswell |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-dxpge2x/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-zoqwa7c/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-habm2fz/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-drachcs/ | | \\\u2500 gcc /9.4.0/ icelake |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-lih7mwq/ | \\\u2500 gcc /9.4.0/ skylake_avx512 |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-ad5zrqt/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-cuu4pwk/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-lih7mwq/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-usztzez/ | | \\\u2500 gcc /9.4.0/ zen |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-364ncu6/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-wr2rrpa/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-bonsmsu/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-7t7xybh/ | | \\\u2500 gcc /9.4.0/ zen2 |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-vwuo3ee/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-mf2vweb/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-velsqdk/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-iycblyc/ | | \\\u2500 gcc /9.4.0/ Note intel refers to the classic intel compilers ( icc , ifort , icpc , ...). The intel-oneapi-compilers-classic module adds intel to modulepath. Hierarchy naming scheme \u00b6 linux-ubuntu20.04-[architecture]/[compiler]/[compiler-version]/ [module-name]/[version] linux-ubuntu20.04-[architecture]/[provider]/[provider-version]/[compiler]/[compiler-version]/ [module-name]/[version] Nested providers are possible. Each of these paths have a prefix of /modules/spack/share/spack/lmod/ . How to use the hierarchy \u00b6 You can find modules anywhere in the hierarchy with the unity-module-find command. From the full path of your desired module you should be able to tell which other modules need to be loaded first. Example: \u00b6 user@login1:~$ module load gromacs No module(s) or extension(s) found! If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". user@login1:~$ unity-module-find gromacs Modules found: linux-ubuntu20.04-cascadelake/openmpi/4.1.3-lih7mwq/intel-mkl/2020.4.304-w2r5zyv/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-haswell/openmpi/4.1.3-habm2fz/intel-mkl/2020.4.304-drachcs/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-skylake_avx512/openmpi/4.1.3-lsrnpnm/intel-mkl/2020.4.304-usztzez/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-x86_64/intel-oneapi-mpi/2021.6.0-h3cppyo/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-x86_64/openmpi/4.1.3-3rgk3nu/intel-mkl/2020.4.304-gmusbfh/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-zen/openmpi/4.1.3-bonsmsu/intel-mkl/2020.4.304-7t7xybh/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-zen2/openmpi/4.1.3-velsqdk/intel-mkl/2020.4.304-iycblyc/gcc/9.4.0/ gromacs/2021.3 This is the module that I want: linux-ubuntu20.04-x86_64/ openmpi /4.1.3-3rgk3nu/ intel-mkl /2020.4.304-gmusbfh/ gcc /9.4.0/ gromacs /2021.3 In its path I can see openmpi , intel-mkl , gcc , and gromacs . Each of these are modules. gcc is loaded by default, so I can ignore it. I can load the other modules in series in one line: $ module load openmpi intel-mkl gromacs Micro-architecture \u00b6 Micro-architecture specific modules are optimized for particular CPU's, but can cause problems if used on other CPU's. If you want better performance for your job you can use these modules, but you should also use slurm constraints so that you always get a node with the correct type of CPU. Slurm constraints \u00b6 You can see a list of all possible constraints with the unity-slurm-list-constraints command. You can see a list of nodes that meet a given constraint with the unity-slurm-find-nodes command. You can see possible constraints for each node on our node list . sbatch script: #SBATCH -C linux-ubuntu20.04-skylake_avx512 module load microarch/skylake_avx512 ... srun interactive session: srun --pty -C linux-ubuntu20.04-skylake_avx512 bash module load microarch/skylake_avx512 ... Opt Out \u00b6 While you can't put the directories back together, you can use the 'flat' module view rather than the limited view. This will add every directory of the hierarchy to your modulepath in an arbitrary order. There are problems with the flat view, like module name conflicts . This is not recommended , and support will not be given for issues with the flat view. To enable the flat view: export LMOD_ENABLE_LIMITED_VIEW=false And to make the change persist: echo \"export LMOD_ENABLE_LIMITED_VIEW=false\" >> ~/.bashrc The change will be applied on your next login shell. You can also reload the modulepath in your current shell: source /etc/profile Learn more \u00b6 About the Hierarchy Change https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy https://lmod.readthedocs.io/en/latest/080_hierarchy.html","title":"Hierarchy"},{"location":"software/module-hierarchy.html#module-hierarchy","text":"Environment Modules is a tool to change dynamically what software is available for use by a given user at a given time. Before you read this, it's recommended that you first read the introduction and the module usage guide . As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. Our strategy is to create a module hierarchy to divide modules by their stacks. The modulepath environment variable $MODULEPATH is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default. This means not all modules can be found with module avail by default.","title":"Module Hierarchy"},{"location":"software/module-hierarchy.html#here-is-the-full-unity-module-hierarchy-as-of-20221026","text":"Compilers are red, providers are blue, and default directories are bold. /modules/modulefiles/ x86_64 |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-h3cppyo/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-6pbqv7b/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-3rgk3nu/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-gmusbfh/ | | \\\u2500 gcc /9.4.0/ cascadelake |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-ad5zrqt/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-cuu4pwk/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-lih7mwq/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-w2r5zyv/ | | \\\u2500 gcc /9.4.0/ haswell |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-dxpge2x/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-zoqwa7c/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-habm2fz/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-drachcs/ | | \\\u2500 gcc /9.4.0/ icelake |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-lih7mwq/ | \\\u2500 gcc /9.4.0/ skylake_avx512 |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-ad5zrqt/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-cuu4pwk/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-lih7mwq/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-usztzez/ | | \\\u2500 gcc /9.4.0/ zen |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-364ncu6/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-wr2rrpa/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-bonsmsu/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-7t7xybh/ | | \\\u2500 gcc /9.4.0/ zen2 |\u2500\u2500 gcc /9.4.0/ |\u2500\u2500 intel /2021.4/ |\u2500\u2500 intel-oneapi-mpi /2021.6.0-vwuo3ee/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openblas /0.3.18-mf2vweb/ | \\\u2500 gcc /9.4.0/ |\u2500\u2500 openmpi /4.1.3-velsqdk/ | |\u2500\u2500 gcc /9.4.0/ | |\u2500\u2500 intel-mkl /2020.4.304-iycblyc/ | | \\\u2500 gcc /9.4.0/ Note intel refers to the classic intel compilers ( icc , ifort , icpc , ...). The intel-oneapi-compilers-classic module adds intel to modulepath.","title":"Here is the full Unity module hierarchy as of 2022/10/26:"},{"location":"software/module-hierarchy.html#hierarchy-naming-scheme","text":"linux-ubuntu20.04-[architecture]/[compiler]/[compiler-version]/ [module-name]/[version] linux-ubuntu20.04-[architecture]/[provider]/[provider-version]/[compiler]/[compiler-version]/ [module-name]/[version] Nested providers are possible. Each of these paths have a prefix of /modules/spack/share/spack/lmod/ .","title":"Hierarchy naming scheme"},{"location":"software/module-hierarchy.html#how-to-use-the-hierarchy","text":"You can find modules anywhere in the hierarchy with the unity-module-find command. From the full path of your desired module you should be able to tell which other modules need to be loaded first.","title":"How to use the hierarchy"},{"location":"software/module-hierarchy.html#example","text":"user@login1:~$ module load gromacs No module(s) or extension(s) found! If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". user@login1:~$ unity-module-find gromacs Modules found: linux-ubuntu20.04-cascadelake/openmpi/4.1.3-lih7mwq/intel-mkl/2020.4.304-w2r5zyv/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-haswell/openmpi/4.1.3-habm2fz/intel-mkl/2020.4.304-drachcs/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-skylake_avx512/openmpi/4.1.3-lsrnpnm/intel-mkl/2020.4.304-usztzez/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-x86_64/intel-oneapi-mpi/2021.6.0-h3cppyo/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-x86_64/openmpi/4.1.3-3rgk3nu/intel-mkl/2020.4.304-gmusbfh/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-zen/openmpi/4.1.3-bonsmsu/intel-mkl/2020.4.304-7t7xybh/gcc/9.4.0/ gromacs/2021.3 linux-ubuntu20.04-zen2/openmpi/4.1.3-velsqdk/intel-mkl/2020.4.304-iycblyc/gcc/9.4.0/ gromacs/2021.3 This is the module that I want: linux-ubuntu20.04-x86_64/ openmpi /4.1.3-3rgk3nu/ intel-mkl /2020.4.304-gmusbfh/ gcc /9.4.0/ gromacs /2021.3 In its path I can see openmpi , intel-mkl , gcc , and gromacs . Each of these are modules. gcc is loaded by default, so I can ignore it. I can load the other modules in series in one line: $ module load openmpi intel-mkl gromacs","title":"Example:"},{"location":"software/module-hierarchy.html#micro-architecture","text":"Micro-architecture specific modules are optimized for particular CPU's, but can cause problems if used on other CPU's. If you want better performance for your job you can use these modules, but you should also use slurm constraints so that you always get a node with the correct type of CPU.","title":"Micro-architecture"},{"location":"software/module-hierarchy.html#slurm-constraints","text":"You can see a list of all possible constraints with the unity-slurm-list-constraints command. You can see a list of nodes that meet a given constraint with the unity-slurm-find-nodes command. You can see possible constraints for each node on our node list . sbatch script: #SBATCH -C linux-ubuntu20.04-skylake_avx512 module load microarch/skylake_avx512 ... srun interactive session: srun --pty -C linux-ubuntu20.04-skylake_avx512 bash module load microarch/skylake_avx512 ...","title":"Slurm constraints"},{"location":"software/module-hierarchy.html#opt-out","text":"While you can't put the directories back together, you can use the 'flat' module view rather than the limited view. This will add every directory of the hierarchy to your modulepath in an arbitrary order. There are problems with the flat view, like module name conflicts . This is not recommended , and support will not be given for issues with the flat view. To enable the flat view: export LMOD_ENABLE_LIMITED_VIEW=false And to make the change persist: echo \"export LMOD_ENABLE_LIMITED_VIEW=false\" >> ~/.bashrc The change will be applied on your next login shell. You can also reload the modulepath in your current shell: source /etc/profile","title":"Opt Out"},{"location":"software/module-hierarchy.html#learn-more","text":"About the Hierarchy Change https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy https://lmod.readthedocs.io/en/latest/080_hierarchy.html","title":"Learn more"},{"location":"software/module-intro.html","text":"Intro. to Environment Modules \u00b6 As a Unity user, you have access to a wide variety of software. Making all of this software available simultaneously and without conflicts is a complex problem, and the solution is Environment modules. An environment is a set of shell variables of the form \"KEY=VALUE\" . Note You can see your current environment using the env command. The PATH environment variable \u00b6 $PATH is a list of directories (folders) delimited by colons. $ echo $PATH /usr/local/bin:/usr/bin:/bin Most of the commands you use in the shell are actually executable files somewhere on the filesystem. When you enter a command, the shell searches the directories in $PATH (from left to right) for an executable by that name. If there are multiple executables of the same name, whichever is found earlier in the $PATH (further to the left) is used. This means that the commands available in your shell can be changed by changing your environment. Note These executable files are called binaries . This is why they are commonly kept in a bin/ directory. Modules \u00b6 Environment modules are scripts that modify your environment. We use modules to add new directories to your $PATH , making the executables within available for use. We 'prepend' the $PATH , making this new directory furthest to the left. This makes sure that the executables within are chosen first by the shell when you call their name. $ which python3 /usr/bin/python3 $ module load python/3.9.1 $ which python3 /modules/apps/python/3.9.1/bin/python3 $ echo $PATH /usr/local/bin:/usr/bin:/bin $ module load python/3.9.1 $ echo $PATH /modules/apps/python/3.9.1/bin:/usr/local/bin:/usr/bin:/bin","title":"Introduction"},{"location":"software/module-intro.html#intro-to-environment-modules","text":"As a Unity user, you have access to a wide variety of software. Making all of this software available simultaneously and without conflicts is a complex problem, and the solution is Environment modules. An environment is a set of shell variables of the form \"KEY=VALUE\" . Note You can see your current environment using the env command.","title":"Intro. to Environment Modules"},{"location":"software/module-intro.html#the-path-environment-variable","text":"$PATH is a list of directories (folders) delimited by colons. $ echo $PATH /usr/local/bin:/usr/bin:/bin Most of the commands you use in the shell are actually executable files somewhere on the filesystem. When you enter a command, the shell searches the directories in $PATH (from left to right) for an executable by that name. If there are multiple executables of the same name, whichever is found earlier in the $PATH (further to the left) is used. This means that the commands available in your shell can be changed by changing your environment. Note These executable files are called binaries . This is why they are commonly kept in a bin/ directory.","title":"The PATH environment variable"},{"location":"software/module-intro.html#modules","text":"Environment modules are scripts that modify your environment. We use modules to add new directories to your $PATH , making the executables within available for use. We 'prepend' the $PATH , making this new directory furthest to the left. This makes sure that the executables within are chosen first by the shell when you call their name. $ which python3 /usr/bin/python3 $ module load python/3.9.1 $ which python3 /modules/apps/python/3.9.1/bin/python3 $ echo $PATH /usr/local/bin:/usr/bin:/bin $ module load python/3.9.1 $ echo $PATH /modules/apps/python/3.9.1/bin:/usr/local/bin:/usr/bin:/bin","title":"Modules"},{"location":"software/module-usage.html","text":"Using Environment Modules \u00b6 Modules are easy to use. You can load and unload them as you please, enabling and disabling different software. You can list currently active modules with module list , search for modules with module av , and remove all modules with module purge . While the modules work on the login nodes, the login nodes have strict CPU and memory limits. Jobs that do heavy lifting should always be scheduled through Slurm . List All Available Modules \u00b6 These all do the same thing: module available module avail module av ml av This will return an output that looks something like this: ------------------------------------------------- /modules/modulefiles ------------------------------------------------- R/3.6.2 cuda/10.1.243 (D) gcc/9.2.0 julia/1.1.1 openmpi/4.0.4 cmake/3.7.2 cuda/11.0.1 glxgears/1.0 jupyter/3.6.8 python/2.7.16 cmake/3.15.0 (D) fd3dspher/1.0 gmsh/4.4.1 mathematica/12.0 python/3.7.4 (D) cuda/8.0.61 gcc/5.5.0 gpu-burn/default mesa/19.0.8 qt/5.13.1 cuda/9.0.176 gcc/6.5.0 gromacs/2020.2C miniconda/3.7 stress/1.0.4 cuda/9.2.148 gcc/7.4.0 (D) gromacs/2020.2G (D) opencl/2.2.11 vtk/8.2.0 Note The module list is rapidly growing. To see the full module list, use this command in your terminal. Search for Modules \u00b6 module av gcc This filters the output of module av for just the gcc modules. Loading Modules \u00b6 module load gcc/9.2.0 Note The naming convention for modules is <name>/<version> . Modules built with non-standard software stacks require extra steps to load. They can be found with the unity-module-find command. See module hierarchy . Unloading Modules \u00b6 module unload gcc Unloading All Modules \u00b6 module purge List Currently Loaded Modules \u00b6 module list","title":"Basic Usage"},{"location":"software/module-usage.html#using-environment-modules","text":"Modules are easy to use. You can load and unload them as you please, enabling and disabling different software. You can list currently active modules with module list , search for modules with module av , and remove all modules with module purge . While the modules work on the login nodes, the login nodes have strict CPU and memory limits. Jobs that do heavy lifting should always be scheduled through Slurm .","title":"Using Environment Modules"},{"location":"software/module-usage.html#list-all-available-modules","text":"These all do the same thing: module available module avail module av ml av This will return an output that looks something like this: ------------------------------------------------- /modules/modulefiles ------------------------------------------------- R/3.6.2 cuda/10.1.243 (D) gcc/9.2.0 julia/1.1.1 openmpi/4.0.4 cmake/3.7.2 cuda/11.0.1 glxgears/1.0 jupyter/3.6.8 python/2.7.16 cmake/3.15.0 (D) fd3dspher/1.0 gmsh/4.4.1 mathematica/12.0 python/3.7.4 (D) cuda/8.0.61 gcc/5.5.0 gpu-burn/default mesa/19.0.8 qt/5.13.1 cuda/9.0.176 gcc/6.5.0 gromacs/2020.2C miniconda/3.7 stress/1.0.4 cuda/9.2.148 gcc/7.4.0 (D) gromacs/2020.2G (D) opencl/2.2.11 vtk/8.2.0 Note The module list is rapidly growing. To see the full module list, use this command in your terminal.","title":"List All Available Modules"},{"location":"software/module-usage.html#search-for-modules","text":"module av gcc This filters the output of module av for just the gcc modules.","title":"Search for Modules"},{"location":"software/module-usage.html#loading-modules","text":"module load gcc/9.2.0 Note The naming convention for modules is <name>/<version> . Modules built with non-standard software stacks require extra steps to load. They can be found with the unity-module-find command. See module hierarchy .","title":"Loading Modules"},{"location":"software/module-usage.html#unloading-modules","text":"module unload gcc","title":"Unloading Modules"},{"location":"software/module-usage.html#unloading-all-modules","text":"module purge","title":"Unloading All Modules"},{"location":"software/module-usage.html#list-currently-loaded-modules","text":"module list","title":"List Currently Loaded Modules"},{"location":"software/side-notes.html","text":"An environment can also include aliases and functions. These can shortcut the previously mentioned process of $PATH searching. Note You can list all shell builtins with the compgen -b command. You can list all functions with the declare -F command. You can list all aliases with the alias command. You can see the definition of a function or alias with the type command. Note ml is a shortcut. On its own it means module list . Combined with the name of a module it means module load . Combined with other module commands it simply means module .","title":"Side notes"},{"location":"technical/nodelist.html","text":"Node List \u00b6 The Unity cluster is a heterogeneous cluster. We plan to keep it a heterogeneous cluster. To clear confusion, a node list is below, with slurm constraints to refine your node selection. You can use the -C flag in your slurm jobs to batch your jobs to a specific set of nodes. If you don't include a constraint, your job may land in any number of nodes in the partition. You can see a list of all possible constraints with the unity-slurm-list-constraints command. You can see a list of nodes that meet a given constraint with the unity-slurm-find-nodes command. CPU Nodes \u00b6 Name Model CPU (Cores and Threads Per CPU) RAM Partitions Constraints cpu[001-008] Lenovo ThinkSystem SD530 2x Intel Xeon Gold 6126 (12 Cores 24 Threads) 192 GiB (Node(s) 15-8) 384 GiB (Nodes 2-4) cpu cpu-long len-sd530_2018 avx512 intel linux-ubuntu20.04-skylake_avx512 cpu[013-025] Dell Poweredge R640 2x Intel Xeon Gold 6148 (20 Cores 40 Threads) 192 GiB cpu cpu-long dell-r640_2020</li avx512 intel linux-ubuntu20.04-skylake_avx512 ceewater-cpu[001-007] Lenovo ThinkSystem SR635 1x AMD EPYC-Rome 7402 (24 Cores 48 Threads) 128 GiB ceewater_cjgleason-cpu ceewater_casey-cpu ceewater_kandread-cpu cpu-preempt ceewater_len-sr635_2020 amd linux-ubuntu20.04-zen2 astroth-cpu[001-008] SuperMicro SBI-4429P 2x Xeon Silver 4215R (8 Cores 16 Threads) 192 GiB astroth-cpu cpu-preempt astroth_smicro-sbi4429p_2021 avx512 intel linux-ubuntu20.04-cascadelake zhoulin-cpu[001-006] Lenovo SR645 2x AMD EPYC 7702 (64 Cores 128 Threads) 512 GiB zhoulin-cpu cpu-preempt zhoulin_len-sr645_2021 amd linux-ubuntu20.04-zen2 toltec-cpu[001-006] Dell R640 2x Intel Xeon Gold 5218 CPU (32 Cores 64 Threads) 383 GiB toltec-cpu cpu-preempt toltec_dell-r640_2021 avx512 intel linux-ubuntu20.04-cascadelake gaoseismolab-cpu[001-005] Lenovo SR630 v2 2x Intel Xeon Platinum 8358 (32 Cores 64 Threads) 512 GiB gaoseismolab-cpu cpu-preempt avx512 intel linux-ubuntu20.04-icelake uri-cpu[001-005] Intel S2600BPB 2x Intel Xeon Gold 6238R (28 Cores 56 Threads) 512 GiB uri-cpu cpu cpu-long avx512 intel linux-ubuntu20.04-cascadelake uri-cpu[006-021] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 128 GiB cpu cpu-long avx512 intel linux-ubuntu20.04-icelake uri-cpu[022-037] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 128 GiB cpu-preempt uri-cpu avx512 intel linux-ubuntu20.04-icelake uri-cpu[038-045] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 1024 GiB cpu-preempt uri-cpu avx512 intel linux-ubuntu20.04-icelake uri-cpu[046-047] PowerNV 8335-GTH POWER9 (32 cores, 128 threads) 256 GiB uri-power9 power9le altivec ppc64le GPU Nodes \u00b6 Name Model CPU GPU RAM Partitions Constraints gpu[001-002] Lenovo ThinkSystem SR650 2x Intel Xeon Silver 4110 (8 Cores 16 Threads) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB gpu gpu-long len-sr650_2018 avx512 v100 intel linux-ubuntu20.04-skylake_avx512 gpu[003-004] Dell Poweredge R740 2x Intel Xeon Gold 6140 (18 Cores 36 Threads) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB gpu gpu-long len-sr650_2018 avx512 v100 intel linux-ubuntu20.04-skylake_avx512 ials-gpu[001-033] Atipa 2x Intel Xeon Silver 4214R (12 Cores 12 Threads) 8x NVIDIA RTX 2080ti (12GB VRAM) 192 GiB ials-gpu gpu gpu-long ials_gigabyte_2020 avx512 2080ti intel linux-ubuntu20.04-cascadelake astroth-gpu[001-003] ASRock AMD Ryzen Threadripper 1900X (8 Cores 16 Threads) 2x NVIDIA RTX 2080 (8GB VRAM) 32 GiB astroth-gpu astro_asrock_x399_2020 2080 amd linux-ubuntu20.04-zen ece-gpu[001-002] Lenovo SR670 2x Intel Xeon Gold 6226R CPU (64 Cores 128 Threads) 4x NVIDIA Tesla A100 (40GB VRAM) 384 GiB ece-gpu ece_len-sr670_2021 avx512 a100 intel gypsum-gpu[001-025] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores 24 Threads) 4x NVIDIA Tesla M40 (24GB VRAM) 256 GiB gypsum-m40 gpu-preempt linux-ubuntu20.04-haswell gypsum-gpu[026-099] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores 24 Threads) 4x NVIDIA GeForce GTX TITAN X (12GB VRAM) 256 GiB gypsum-titanx gpu-preempt linux-ubuntu20.04-haswell gypsum-gpu[104-156] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores 48 Threads) 8x NVIDIA GeForce GTX 1080 Ti (11GB VRAM) 384 GiB gypsum-1080ti gpu-preempt linux-ubuntu20.04-skylake_avx512 gypsum-gpu[157-181] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores 48 Threads) 8x NVIDIA GeForce GTX 2080 Ti (11GB VRAM) 384 GiB gypsum-2080ti gpu-preempt linux-ubuntu20.04-skylake_avx512 gypsum-gpu[182-189] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores 48 Threads) 8x NVIDIA Quadro RTX 8000 (48GB VRAM) 384 GiB gypsum-rtx8000 gpu-preempt linux-ubuntu20.04-skylake_avx512 gypsum-gpu[190-192] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores 48 Threads) 8x NVIDIA GeForce GTX 2080 Ti (11GB VRAM) 384 GiB gypsum-2080ti gpu-preempt linux-ubuntu20.04-skylake_avx512 uri-gpu[001-004] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 4x NVIDIA Tesla A100 (80GB VRAM) 128 GiB gpu gpu-long avx512 intel linux-ubuntu20.04-icelake a100 uri-gpu[005-008] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 4x NVIDIA Tesla A100 (80GB VRAM) 128 GiB gpu-preempt uri-gpu avx512 intel linux-ubuntu20.04-icelake a100","title":"Node List"},{"location":"technical/nodelist.html#node-list","text":"The Unity cluster is a heterogeneous cluster. We plan to keep it a heterogeneous cluster. To clear confusion, a node list is below, with slurm constraints to refine your node selection. You can use the -C flag in your slurm jobs to batch your jobs to a specific set of nodes. If you don't include a constraint, your job may land in any number of nodes in the partition. You can see a list of all possible constraints with the unity-slurm-list-constraints command. You can see a list of nodes that meet a given constraint with the unity-slurm-find-nodes command.","title":"Node List"},{"location":"technical/nodelist.html#cpu-nodes","text":"Name Model CPU (Cores and Threads Per CPU) RAM Partitions Constraints cpu[001-008] Lenovo ThinkSystem SD530 2x Intel Xeon Gold 6126 (12 Cores 24 Threads) 192 GiB (Node(s) 15-8) 384 GiB (Nodes 2-4) cpu cpu-long len-sd530_2018 avx512 intel linux-ubuntu20.04-skylake_avx512 cpu[013-025] Dell Poweredge R640 2x Intel Xeon Gold 6148 (20 Cores 40 Threads) 192 GiB cpu cpu-long dell-r640_2020</li avx512 intel linux-ubuntu20.04-skylake_avx512 ceewater-cpu[001-007] Lenovo ThinkSystem SR635 1x AMD EPYC-Rome 7402 (24 Cores 48 Threads) 128 GiB ceewater_cjgleason-cpu ceewater_casey-cpu ceewater_kandread-cpu cpu-preempt ceewater_len-sr635_2020 amd linux-ubuntu20.04-zen2 astroth-cpu[001-008] SuperMicro SBI-4429P 2x Xeon Silver 4215R (8 Cores 16 Threads) 192 GiB astroth-cpu cpu-preempt astroth_smicro-sbi4429p_2021 avx512 intel linux-ubuntu20.04-cascadelake zhoulin-cpu[001-006] Lenovo SR645 2x AMD EPYC 7702 (64 Cores 128 Threads) 512 GiB zhoulin-cpu cpu-preempt zhoulin_len-sr645_2021 amd linux-ubuntu20.04-zen2 toltec-cpu[001-006] Dell R640 2x Intel Xeon Gold 5218 CPU (32 Cores 64 Threads) 383 GiB toltec-cpu cpu-preempt toltec_dell-r640_2021 avx512 intel linux-ubuntu20.04-cascadelake gaoseismolab-cpu[001-005] Lenovo SR630 v2 2x Intel Xeon Platinum 8358 (32 Cores 64 Threads) 512 GiB gaoseismolab-cpu cpu-preempt avx512 intel linux-ubuntu20.04-icelake uri-cpu[001-005] Intel S2600BPB 2x Intel Xeon Gold 6238R (28 Cores 56 Threads) 512 GiB uri-cpu cpu cpu-long avx512 intel linux-ubuntu20.04-cascadelake uri-cpu[006-021] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 128 GiB cpu cpu-long avx512 intel linux-ubuntu20.04-icelake uri-cpu[022-037] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 128 GiB cpu-preempt uri-cpu avx512 intel linux-ubuntu20.04-icelake uri-cpu[038-045] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 1024 GiB cpu-preempt uri-cpu avx512 intel linux-ubuntu20.04-icelake uri-cpu[046-047] PowerNV 8335-GTH POWER9 (32 cores, 128 threads) 256 GiB uri-power9 power9le altivec ppc64le","title":"CPU Nodes"},{"location":"technical/nodelist.html#gpu-nodes","text":"Name Model CPU GPU RAM Partitions Constraints gpu[001-002] Lenovo ThinkSystem SR650 2x Intel Xeon Silver 4110 (8 Cores 16 Threads) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB gpu gpu-long len-sr650_2018 avx512 v100 intel linux-ubuntu20.04-skylake_avx512 gpu[003-004] Dell Poweredge R740 2x Intel Xeon Gold 6140 (18 Cores 36 Threads) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB gpu gpu-long len-sr650_2018 avx512 v100 intel linux-ubuntu20.04-skylake_avx512 ials-gpu[001-033] Atipa 2x Intel Xeon Silver 4214R (12 Cores 12 Threads) 8x NVIDIA RTX 2080ti (12GB VRAM) 192 GiB ials-gpu gpu gpu-long ials_gigabyte_2020 avx512 2080ti intel linux-ubuntu20.04-cascadelake astroth-gpu[001-003] ASRock AMD Ryzen Threadripper 1900X (8 Cores 16 Threads) 2x NVIDIA RTX 2080 (8GB VRAM) 32 GiB astroth-gpu astro_asrock_x399_2020 2080 amd linux-ubuntu20.04-zen ece-gpu[001-002] Lenovo SR670 2x Intel Xeon Gold 6226R CPU (64 Cores 128 Threads) 4x NVIDIA Tesla A100 (40GB VRAM) 384 GiB ece-gpu ece_len-sr670_2021 avx512 a100 intel gypsum-gpu[001-025] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores 24 Threads) 4x NVIDIA Tesla M40 (24GB VRAM) 256 GiB gypsum-m40 gpu-preempt linux-ubuntu20.04-haswell gypsum-gpu[026-099] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores 24 Threads) 4x NVIDIA GeForce GTX TITAN X (12GB VRAM) 256 GiB gypsum-titanx gpu-preempt linux-ubuntu20.04-haswell gypsum-gpu[104-156] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores 48 Threads) 8x NVIDIA GeForce GTX 1080 Ti (11GB VRAM) 384 GiB gypsum-1080ti gpu-preempt linux-ubuntu20.04-skylake_avx512 gypsum-gpu[157-181] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores 48 Threads) 8x NVIDIA GeForce GTX 2080 Ti (11GB VRAM) 384 GiB gypsum-2080ti gpu-preempt linux-ubuntu20.04-skylake_avx512 gypsum-gpu[182-189] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores 48 Threads) 8x NVIDIA Quadro RTX 8000 (48GB VRAM) 384 GiB gypsum-rtx8000 gpu-preempt linux-ubuntu20.04-skylake_avx512 gypsum-gpu[190-192] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores 48 Threads) 8x NVIDIA GeForce GTX 2080 Ti (11GB VRAM) 384 GiB gypsum-2080ti gpu-preempt linux-ubuntu20.04-skylake_avx512 uri-gpu[001-004] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 4x NVIDIA Tesla A100 (80GB VRAM) 128 GiB gpu gpu-long avx512 intel linux-ubuntu20.04-icelake a100 uri-gpu[005-008] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 cores) 4x NVIDIA Tesla A100 (80GB VRAM) 128 GiB gpu-preempt uri-gpu avx512 intel linux-ubuntu20.04-icelake a100","title":"GPU Nodes"},{"location":"technical/partitionlist.html","text":"Partition List \u00b6 General Use Partitions \u00b6 These are the only partitions that are open to use by all users of Unity. Name Relative Wait Time Default Job Time Time Limit Max CPU's Per Node cpu medium 4 hours 1 day 40.0 cpu-long long 7 days 14 days 40.0 cpu-preempt short 4 hours (see below) 14 days (see below) 256.0 gpu medium 4 hours 1 day gpu-long long 7 days 14 days gpu-preempt short 4 hours (see below) 14 days (see below) Preempt \u00b6 Jobs can be killed and re-queued after two hours in the -preempt partition. To avoid losing progress, it's a good idea to use software that supports checkpointing. Checkpointing in a nutshell is periodically saving the job's state to a file, and having the capability to read this file and resume work from said state. If you don't want your job re-queued (but still killed), you can specify --no-requeue in your job. Gypsum Cluster Partitions \u00b6 Gypsum users, depending on the type, have access to these partitions. Name Time Limit Comments gypsum-m40 7 days M40 GPU partition gypsum-titanx 7 days TITAN X GPU partition gypsum-1080ti 7 days 1080 Ti GPU partition gypsum-2080ti 7 days 2080 Ti GPU partition gypsum-rtx8000 7 days RTX 8000 GPU partition IALS Cluster Partitions \u00b6 If you are an authorized IALS member on Unity, you can use these partitions. Name Time Limit Comments ials-gpu 14 days GPU partition for IALS Other Priority Partitions \u00b6 These are the remaining priority partitions for smaller installations purchased for specific labs by themselves. Name Time Limit Comments ceewater_cjgleason-cpu Unlimited ceewater CPU partition for cjgleason group ceewater_casey-cpu Unlimited ceewater CPU partition for casey group ceewater_kandread-cpu Unlimited ceewater CPU partition for kandread group astroth-cpu Unlimited astroth CPU partition zhoulin-cpu Unlimited zhoulin CPU partition toltec-cpu Unlimited toltec CPU partition gaoseismolab-cpu Unlimited gaoseismolab CPU partition uri-cpu Unlimited URI CPU partition ece-gpu 5 days ece partition for ECE courses only","title":"Partition List"},{"location":"technical/partitionlist.html#partition-list","text":"","title":"Partition List"},{"location":"technical/partitionlist.html#general-use-partitions","text":"These are the only partitions that are open to use by all users of Unity. Name Relative Wait Time Default Job Time Time Limit Max CPU's Per Node cpu medium 4 hours 1 day 40.0 cpu-long long 7 days 14 days 40.0 cpu-preempt short 4 hours (see below) 14 days (see below) 256.0 gpu medium 4 hours 1 day gpu-long long 7 days 14 days gpu-preempt short 4 hours (see below) 14 days (see below)","title":"General Use Partitions"},{"location":"technical/partitionlist.html#preempt","text":"Jobs can be killed and re-queued after two hours in the -preempt partition. To avoid losing progress, it's a good idea to use software that supports checkpointing. Checkpointing in a nutshell is periodically saving the job's state to a file, and having the capability to read this file and resume work from said state. If you don't want your job re-queued (but still killed), you can specify --no-requeue in your job.","title":"Preempt"},{"location":"technical/partitionlist.html#gypsum-cluster-partitions","text":"Gypsum users, depending on the type, have access to these partitions. Name Time Limit Comments gypsum-m40 7 days M40 GPU partition gypsum-titanx 7 days TITAN X GPU partition gypsum-1080ti 7 days 1080 Ti GPU partition gypsum-2080ti 7 days 2080 Ti GPU partition gypsum-rtx8000 7 days RTX 8000 GPU partition","title":"Gypsum Cluster Partitions"},{"location":"technical/partitionlist.html#ials-cluster-partitions","text":"If you are an authorized IALS member on Unity, you can use these partitions. Name Time Limit Comments ials-gpu 14 days GPU partition for IALS","title":"IALS Cluster Partitions"},{"location":"technical/partitionlist.html#other-priority-partitions","text":"These are the remaining priority partitions for smaller installations purchased for specific labs by themselves. Name Time Limit Comments ceewater_cjgleason-cpu Unlimited ceewater CPU partition for cjgleason group ceewater_casey-cpu Unlimited ceewater CPU partition for casey group ceewater_kandread-cpu Unlimited ceewater CPU partition for kandread group astroth-cpu Unlimited astroth CPU partition zhoulin-cpu Unlimited zhoulin CPU partition toltec-cpu Unlimited toltec CPU partition gaoseismolab-cpu Unlimited gaoseismolab CPU partition uri-cpu Unlimited URI CPU partition ece-gpu 5 days ece partition for ECE courses only","title":"Other Priority Partitions"},{"location":"technical/storage.html","text":"Storage \u00b6 Below if a table of all available storage on Unity. Mountpoint Name Location Type Quota Description /home Home directories Everywhere HDD 50 GB Home directories should be used only for user init files. /work/pi_ Work directories Everywhere SSD 1 TB Work should be used as the primary location for running cluster jobs. This is a shared folder for all users in the PI group. /work/username, is a legacy directory available to older users which is being phased out. /project Project directories Everywhere HDD As Needed Project directories are available to PI's upon request. Good for large dataset storage or any larger storage that is not directly used for job I/O. PI's should e-mail hpc@umass.edu to request. A common use case is generating job output in /work and copying to permanent storage in /project afterwards. Not for job I/O /nese NESE mounts Everywhere HDD/Tape Varying Legacy images available from the northeast storage exchange can be found here. Not for job I/O /nas Buy-in NAS mounts Everywhere Varying Varying Legacy location where the mounts for buy-in NAS hardware are located on Unity. For users who purchased storage nodes for their own use on Unity only. /scratch Scratch space Everywhere (Intended for Compute) SSD 40 TB / user, cleared at the end of job /scratch/[nodeid]/[jobid] is created when a job is started. That folder is assigned to $TMP and deleted after the job is complete. This directory is not directly available to users. /gypsum Gypsum devices Everywhere HDD Varying For users migrating from the Gypsum cluster to the Unity clusters, you will find all your old storage here. /old Old mounts Everywhere (Read-Only) Varying Varying Old filesystems which are deprecated live here until they are deleted.","title":"Storage"},{"location":"technical/storage.html#storage","text":"Below if a table of all available storage on Unity. Mountpoint Name Location Type Quota Description /home Home directories Everywhere HDD 50 GB Home directories should be used only for user init files. /work/pi_ Work directories Everywhere SSD 1 TB Work should be used as the primary location for running cluster jobs. This is a shared folder for all users in the PI group. /work/username, is a legacy directory available to older users which is being phased out. /project Project directories Everywhere HDD As Needed Project directories are available to PI's upon request. Good for large dataset storage or any larger storage that is not directly used for job I/O. PI's should e-mail hpc@umass.edu to request. A common use case is generating job output in /work and copying to permanent storage in /project afterwards. Not for job I/O /nese NESE mounts Everywhere HDD/Tape Varying Legacy images available from the northeast storage exchange can be found here. Not for job I/O /nas Buy-in NAS mounts Everywhere Varying Varying Legacy location where the mounts for buy-in NAS hardware are located on Unity. For users who purchased storage nodes for their own use on Unity only. /scratch Scratch space Everywhere (Intended for Compute) SSD 40 TB / user, cleared at the end of job /scratch/[nodeid]/[jobid] is created when a job is started. That folder is assigned to $TMP and deleted after the job is complete. This directory is not directly available to users. /gypsum Gypsum devices Everywhere HDD Varying For users migrating from the Gypsum cluster to the Unity clusters, you will find all your old storage here. /old Old mounts Everywhere (Read-Only) Varying Varying Old filesystems which are deprecated live here until they are deleted.","title":"Storage"},{"location":"transfers/transfers.html","text":"Transferring data from MGHPCC to Unity: \u00b6 This guide will allow you to transfer data from the MGHPCC to Unity via Globus. \u00b6 Step 1: Log in to Globus \u00b6 Visit this site: https://app.globus.org/file-manager Use your university credentials to log in Step 2: Connect to the MGHPCC and Unity \u00b6 Select the File Manager tab from lefthand pane In the two \"Collection\" boxes, select the MGHPCC and Unity via their Globus identities The MGHPCC is ghpcc#ghpcc07 Unity is Unity more technical identifier: acda5457-9c06-4564-8375-260ba428f22a If you start typing either in the \"Collection\" text box, the dropdown will update and allow you to select the appropriate \"Collection\" (aka cluster) Provide your login credentials for each cluster Once you're finished logging in to each connection, the screen should look something like this: Step 3: Transfer files \u00b6 Below, the Connection and Path text boxes, you should see the contents of each directory. Navigate to the location of the file/directory you'd like to transfer from and to. Select the file/directory you'd like to transfer Click on the \"Transfer or Sync to...\" button. Shown in the middle here: Voila! The file/directory will be queued up and transferred shortly! Step 4: Confirm that transfer was successful \u00b6 You'll receive an email once the transfer occurs. Open the Destination directory and confirm that the file was transferred as expected. This guide will allow you to send data from the MGHPCC to the Unity server using scp for the purposes of transitioning. \u00b6 Notes: \u00b6 You will need to have set up an acount on the Unity server. You will need your password for the MGHPCC server (unless you are using private/public keys for logging on to this too). Step 1. \u00b6 Log in to the MGHPCC. While logged into the MGHPCC, from your home directory, run ssh-keygen -t rsa This will generate a private/public key pair which is unique to each user. The generator will ask you to save the keys as a specific name, and you have the option to associate a password to the keys (recommended but not required). Two files will be created: NAME and NAME.pub NAME.pub is the public key that you will need for Step 2. Step 2. \u00b6 Log in to the Unity cluster portal: https://unity.rc.umass.edu/ Navigate to \"Account Settings\" You will see the SSH keys that are currently linked to your account Click the \"+\" button to add the MGHPCC key to your account. Copy the entire contents of the public key (NAME.pub) from the MGHPCC generated in Step 1 into the prompt. Click \"add key\" Click \"Set Login Shell\" This will now link the two servers, allowing them to communicate Step 3. \u00b6 Log into the Unity cluster Create a file to test the connection between servers (e.g. touch test.txt ) Attempt to transfer the file from Unity to the MGHPCC: scp test.txt username@ghpcc06.umassrc.org:/home/username/ You will be prompted for your password to the MGHPCC - enter it here. If configured properly, this file should transfer immediately to the specficied destination path (in this case /home/username/ ) Step 4. \u00b6 Assuming that Step 3 worked and the file transferred across from Unity to the MGHPCC, you can use this to pull data across: scp username@ghpcc06.umassrc.org:/where/files/are/you/want/* ./destination/ Each time you run scp in this way, you will be prompted for your password. Note that currently this only works when logged in and running commands from Unity, not the MGHPCC. Globus documentation provided by the Molecular Ecology and Conservation Lab, Dept. Environmental Conservation.","title":"File Transfers"},{"location":"transfers/transfers.html#transferring-data-from-mghpcc-to-unity","text":"","title":"Transferring data from MGHPCC to Unity:"},{"location":"transfers/transfers.html#this-guide-will-allow-you-to-transfer-data-from-the-mghpcc-to-unity-via-globus","text":"","title":"This guide will allow you to transfer data from the MGHPCC to Unity via Globus."},{"location":"transfers/transfers.html#step-1-log-in-to-globus","text":"Visit this site: https://app.globus.org/file-manager Use your university credentials to log in","title":"Step 1: Log in to Globus"},{"location":"transfers/transfers.html#step-2-connect-to-the-mghpcc-and-unity","text":"Select the File Manager tab from lefthand pane In the two \"Collection\" boxes, select the MGHPCC and Unity via their Globus identities The MGHPCC is ghpcc#ghpcc07 Unity is Unity more technical identifier: acda5457-9c06-4564-8375-260ba428f22a If you start typing either in the \"Collection\" text box, the dropdown will update and allow you to select the appropriate \"Collection\" (aka cluster) Provide your login credentials for each cluster Once you're finished logging in to each connection, the screen should look something like this:","title":"Step 2: Connect to the MGHPCC and Unity"},{"location":"transfers/transfers.html#step-3-transfer-files","text":"Below, the Connection and Path text boxes, you should see the contents of each directory. Navigate to the location of the file/directory you'd like to transfer from and to. Select the file/directory you'd like to transfer Click on the \"Transfer or Sync to...\" button. Shown in the middle here: Voila! The file/directory will be queued up and transferred shortly!","title":"Step 3: Transfer files"},{"location":"transfers/transfers.html#step-4-confirm-that-transfer-was-successful","text":"You'll receive an email once the transfer occurs. Open the Destination directory and confirm that the file was transferred as expected.","title":"Step 4: Confirm that transfer was successful"},{"location":"transfers/transfers.html#this-guide-will-allow-you-to-send-data-from-the-mghpcc-to-the-unity-server-using-scp-for-the-purposes-of-transitioning","text":"","title":"This guide will allow you to send data from the MGHPCC to the Unity server using scp for the purposes of transitioning."},{"location":"transfers/transfers.html#notes","text":"You will need to have set up an acount on the Unity server. You will need your password for the MGHPCC server (unless you are using private/public keys for logging on to this too).","title":"Notes:"},{"location":"transfers/transfers.html#step-1","text":"Log in to the MGHPCC. While logged into the MGHPCC, from your home directory, run ssh-keygen -t rsa This will generate a private/public key pair which is unique to each user. The generator will ask you to save the keys as a specific name, and you have the option to associate a password to the keys (recommended but not required). Two files will be created: NAME and NAME.pub NAME.pub is the public key that you will need for Step 2.","title":"Step 1."},{"location":"transfers/transfers.html#step-2","text":"Log in to the Unity cluster portal: https://unity.rc.umass.edu/ Navigate to \"Account Settings\" You will see the SSH keys that are currently linked to your account Click the \"+\" button to add the MGHPCC key to your account. Copy the entire contents of the public key (NAME.pub) from the MGHPCC generated in Step 1 into the prompt. Click \"add key\" Click \"Set Login Shell\" This will now link the two servers, allowing them to communicate","title":"Step 2."},{"location":"transfers/transfers.html#step-3","text":"Log into the Unity cluster Create a file to test the connection between servers (e.g. touch test.txt ) Attempt to transfer the file from Unity to the MGHPCC: scp test.txt username@ghpcc06.umassrc.org:/home/username/ You will be prompted for your password to the MGHPCC - enter it here. If configured properly, this file should transfer immediately to the specficied destination path (in this case /home/username/ )","title":"Step 3."},{"location":"transfers/transfers.html#step-4","text":"Assuming that Step 3 worked and the file transferred across from Unity to the MGHPCC, you can use this to pull data across: scp username@ghpcc06.umassrc.org:/where/files/are/you/want/* ./destination/ Each time you run scp in this way, you will be prompted for your password. Note that currently this only works when logged in and running commands from Unity, not the MGHPCC. Globus documentation provided by the Molecular Ecology and Conservation Lab, Dept. Environmental Conservation.","title":"Step 4."},{"location":"updates/index.html","text":"The Module Hierarchy Change (Coming soon!) \u00b6 As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. The hierarchy change is to combat module bloat, simplify module names, and (most importantly) prevent conflicts. For many users, nothing will change, but some will have to take a few extra steps when loading modules going forward. The module command refers to Environment Modules. We use Lmod , which is Environment Modules implemented in Lua. The modulepath environment variable $MODULEPATH is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default. This means not all modules can be found with module avail by default. The information on this page is not critical for proper usage of Unity. We know your time is valuable, so you can always skip to Using the Module Hierarchy to learn how to work with these changes. What does this mean for my workflow? \u00b6 Without extra steps you may encounter the following error: No module(s) or extension(s) found! If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". In this case you can follow the documentation on how to use the hierarchy . In short, you use the unity-module-find command, and from that output you should be able to tell which other modules need to be loaded first. In many cases, it can be as simple as changing module load gromacs into module load openmpi gromacs . Does this affect me? \u00b6 The following modules are no longer in the default modulepath: modules built with the intel classic compilers zlib/1.2.12-intel@2021.4.0 pigz modules that use mpi cgns fftw/3.3.8+openmpi4.1.3 fftw/3.3.10+intel-oneapi-mpi2021.6.0 grace gromacs med mmg netcdf openfoam valgrind modules built for specific CPU micro-architectures cascadelake haswell icelake skylake_avx512 zen zen2 Why? \u00b6 Module name length \u00b6 Historically we have tried to make each module name unique. For example: fftw/3.3.8+openmpi4.1.3 fftw/3.3.8+intel-oneapi-mpi2021.6.0 This has worked well enough, but long module names make it harder for Lmod to display many modules on one screen, because horizontal whitespace can be used for more columns. This module: /modules/.../linux-ubuntu20.04-x86_64/ gromacs/2021.3+openmpi4.1.3-intel@2021.4 would instead become: /modules/.../linux-ubuntu20.04-x86_64/gcc/9.4.0/openmpi/4.1.3/intel/2021.4/ gromacs/2021.3 Module name conflicts \u00b6 If module names are not unique, it can be difficult to choose which module to load. When two modules from different directories have the same name, Lmod will decide which is default and mark it with a (D) flag. The only way to get around this is to manually manipulate $MODULEPATH to exclude certain directories until the desired module is marked as default. With the hierarchy, it's implicit that if you add special modules to your modulepath, you want those special modules to take priority. Module Conflicts \u00b6 Some software (particularly that which uses mpi) needs to be built in a similar manner to its dependencies. NetCDF, for example, will not work unless HDF5 and MPI are built with the same compiler and with certain parameters. With modules specific to CPU micro-architectures, it's possible to compile software on one node that is unable to run on other nodes. For example, linking off of Skylake modules may lead to problems when running on older Haswell nodes. If you try to compile using a set of incompatible modules, bad things can happen. At best, Cmake/Autotools will tell you to use something else; At worst, you can struggle with nebulous compiler errors for hours on end. At runtime the result is largely the same. From the Lmod documentation : it is quite easy to load an incompatible set of modules. Now, it is possible that the system administrators at your site might have set up conflicts to avoid loading mismatched modules. However, using conflicts can be fragile. What happens if a site adds a new compiler such as clang or pgi or a new mpi stack? All those module file conflict statements will have to be updated. A module hierarchy is a robust and elegant solution to avoid module conflicts. What is going on under the hood? \u00b6 Our current module tree has two main directories with many modules inside. $ echo $MODULEPATH /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64:/modules/modulefiles $ ls /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64 | wc -l 343 We are simply splitting it into multiple directories. /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64 \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 intelel/2021.4/ \u251c\u2500\u2500 intel-oneapi-mpi/2021.6.0-ad5zrqt/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 openblas/0.3.18-cuu4pwk/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 openmpi/4.1.3-lih7mwq/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 intel-mkl/2020.4.304-w2r5zyv/ \u251c\u2500\u2500 gcc/9.4.0/ The Unity module set is split in two. /modules/modulefiles contains our homebrew modules, those compiled by hand by the admins. /modules/spack/share/spack/modules/ contains modules created by Spack. /modules/modulefiles is not changing. Certain modules will now add a directory to your modulepath, making the modules in that directory available to load. At the start of each new login shell, only modules built with the system compiler gcc/9.4.0 will be included in the modulepath, as defined in the system-wide login scripts . Also included in this change \u00b6 Every new Slurm job will module purge . You will have to make sure that your jobs load modules all by themselves. This is to ensure that incompatible modules are not used. New commands are available in your shell: unity-module-find unity-module-hierarchy unity-module-hierarchy-help unity-module-lmod-disable-help unity-module-lmod-enable-help unity-slurm-list-constraints unity-slurm-find-nodes We are switching Spack TCL modules to Lua. This shouldn't affect the user experience. We are wrapping the module command with a bit of code which will print a message that things have changed. unity-module-lmod-disable-help is provided to suppress this output. Learn more \u00b6 Using the Module Hierarchy https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy https://lmod.readthedocs.io/en/latest/080_hierarchy.html","title":"Cluster Updates"},{"location":"updates/index.html#the-module-hierarchy-change-coming-soon","text":"As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. The hierarchy change is to combat module bloat, simplify module names, and (most importantly) prevent conflicts. For many users, nothing will change, but some will have to take a few extra steps when loading modules going forward. The module command refers to Environment Modules. We use Lmod , which is Environment Modules implemented in Lua. The modulepath environment variable $MODULEPATH is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default. This means not all modules can be found with module avail by default. The information on this page is not critical for proper usage of Unity. We know your time is valuable, so you can always skip to Using the Module Hierarchy to learn how to work with these changes.","title":"The Module Hierarchy Change (Coming soon!)"},{"location":"updates/index.html#what-does-this-mean-for-my-workflow","text":"Without extra steps you may encounter the following error: No module(s) or extension(s) found! If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". In this case you can follow the documentation on how to use the hierarchy . In short, you use the unity-module-find command, and from that output you should be able to tell which other modules need to be loaded first. In many cases, it can be as simple as changing module load gromacs into module load openmpi gromacs .","title":"What does this mean for my workflow?"},{"location":"updates/index.html#does-this-affect-me","text":"The following modules are no longer in the default modulepath: modules built with the intel classic compilers zlib/1.2.12-intel@2021.4.0 pigz modules that use mpi cgns fftw/3.3.8+openmpi4.1.3 fftw/3.3.10+intel-oneapi-mpi2021.6.0 grace gromacs med mmg netcdf openfoam valgrind modules built for specific CPU micro-architectures cascadelake haswell icelake skylake_avx512 zen zen2","title":"Does this affect me?"},{"location":"updates/index.html#why","text":"","title":"Why?"},{"location":"updates/index.html#module-name-length","text":"Historically we have tried to make each module name unique. For example: fftw/3.3.8+openmpi4.1.3 fftw/3.3.8+intel-oneapi-mpi2021.6.0 This has worked well enough, but long module names make it harder for Lmod to display many modules on one screen, because horizontal whitespace can be used for more columns. This module: /modules/.../linux-ubuntu20.04-x86_64/ gromacs/2021.3+openmpi4.1.3-intel@2021.4 would instead become: /modules/.../linux-ubuntu20.04-x86_64/gcc/9.4.0/openmpi/4.1.3/intel/2021.4/ gromacs/2021.3","title":"Module name length"},{"location":"updates/index.html#module-name-conflicts","text":"If module names are not unique, it can be difficult to choose which module to load. When two modules from different directories have the same name, Lmod will decide which is default and mark it with a (D) flag. The only way to get around this is to manually manipulate $MODULEPATH to exclude certain directories until the desired module is marked as default. With the hierarchy, it's implicit that if you add special modules to your modulepath, you want those special modules to take priority.","title":"Module name conflicts"},{"location":"updates/index.html#module-conflicts","text":"Some software (particularly that which uses mpi) needs to be built in a similar manner to its dependencies. NetCDF, for example, will not work unless HDF5 and MPI are built with the same compiler and with certain parameters. With modules specific to CPU micro-architectures, it's possible to compile software on one node that is unable to run on other nodes. For example, linking off of Skylake modules may lead to problems when running on older Haswell nodes. If you try to compile using a set of incompatible modules, bad things can happen. At best, Cmake/Autotools will tell you to use something else; At worst, you can struggle with nebulous compiler errors for hours on end. At runtime the result is largely the same. From the Lmod documentation : it is quite easy to load an incompatible set of modules. Now, it is possible that the system administrators at your site might have set up conflicts to avoid loading mismatched modules. However, using conflicts can be fragile. What happens if a site adds a new compiler such as clang or pgi or a new mpi stack? All those module file conflict statements will have to be updated. A module hierarchy is a robust and elegant solution to avoid module conflicts.","title":"Module Conflicts"},{"location":"updates/index.html#what-is-going-on-under-the-hood","text":"Our current module tree has two main directories with many modules inside. $ echo $MODULEPATH /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64:/modules/modulefiles $ ls /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64 | wc -l 343 We are simply splitting it into multiple directories. /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64 \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 intelel/2021.4/ \u251c\u2500\u2500 intel-oneapi-mpi/2021.6.0-ad5zrqt/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 openblas/0.3.18-cuu4pwk/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 openmpi/4.1.3-lih7mwq/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 intel-mkl/2020.4.304-w2r5zyv/ \u251c\u2500\u2500 gcc/9.4.0/ The Unity module set is split in two. /modules/modulefiles contains our homebrew modules, those compiled by hand by the admins. /modules/spack/share/spack/modules/ contains modules created by Spack. /modules/modulefiles is not changing. Certain modules will now add a directory to your modulepath, making the modules in that directory available to load. At the start of each new login shell, only modules built with the system compiler gcc/9.4.0 will be included in the modulepath, as defined in the system-wide login scripts .","title":"What is going on under the hood?"},{"location":"updates/index.html#also-included-in-this-change","text":"Every new Slurm job will module purge . You will have to make sure that your jobs load modules all by themselves. This is to ensure that incompatible modules are not used. New commands are available in your shell: unity-module-find unity-module-hierarchy unity-module-hierarchy-help unity-module-lmod-disable-help unity-module-lmod-enable-help unity-slurm-list-constraints unity-slurm-find-nodes We are switching Spack TCL modules to Lua. This shouldn't affect the user experience. We are wrapping the module command with a bit of code which will print a message that things have changed. unity-module-lmod-disable-help is provided to suppress this output.","title":"Also included in this change"},{"location":"updates/index.html#learn-more","text":"Using the Module Hierarchy https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy https://lmod.readthedocs.io/en/latest/080_hierarchy.html","title":"Learn more"},{"location":"updates/hierarchy-change.html","text":"The Module Hierarchy Change (Coming soon!) \u00b6 As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. The hierarchy change is to combat module bloat, simplify module names, and (most importantly) prevent conflicts. For many users, nothing will change, but some will have to take a few extra steps when loading modules going forward. The module command refers to Environment Modules. We use Lmod , which is Environment Modules implemented in Lua. The modulepath environment variable $MODULEPATH is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default. This means not all modules can be found with module avail by default. The information on this page is not critical for proper usage of Unity. We know your time is valuable, so you can always skip to Using the Module Hierarchy to learn how to work with these changes. What does this mean for my workflow? \u00b6 Without extra steps you may encounter the following error: No module(s) or extension(s) found! If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". In this case you can follow the documentation on how to use the hierarchy . In short, you use the unity-module-find command, and from that output you should be able to tell which other modules need to be loaded first. In many cases, it can be as simple as changing module load gromacs into module load openmpi gromacs . Does this affect me? \u00b6 The following modules are no longer in the default modulepath: modules built with the intel classic compilers zlib/1.2.12-intel@2021.4.0 pigz modules that use mpi cgns fftw/3.3.8+openmpi4.1.3 fftw/3.3.10+intel-oneapi-mpi2021.6.0 grace gromacs med mmg netcdf openfoam valgrind modules built for specific CPU micro-architectures cascadelake haswell icelake skylake_avx512 zen zen2 Why? \u00b6 Module name length \u00b6 Historically we have tried to make each module name unique. For example: fftw/3.3.8+openmpi4.1.3 fftw/3.3.8+intel-oneapi-mpi2021.6.0 This has worked well enough, but long module names make it harder for Lmod to display many modules on one screen, because horizontal whitespace can be used for more columns. This module: /modules/.../linux-ubuntu20.04-x86_64/ gromacs/2021.3+openmpi4.1.3-intel@2021.4 would instead become: /modules/.../linux-ubuntu20.04-x86_64/gcc/9.4.0/openmpi/4.1.3/intel/2021.4/ gromacs/2021.3 Module name conflicts \u00b6 If module names are not unique, it can be difficult to choose which module to load. When two modules from different directories have the same name, Lmod will decide which is default and mark it with a (D) flag. The only way to get around this is to manually manipulate $MODULEPATH to exclude certain directories until the desired module is marked as default. With the hierarchy, it's implicit that if you add special modules to your modulepath, you want those special modules to take priority. Module Conflicts \u00b6 Some software (particularly that which uses mpi) needs to be built in a similar manner to its dependencies. NetCDF, for example, will not work unless HDF5 and MPI are built with the same compiler and with certain parameters. With modules specific to CPU micro-architectures, it's possible to compile software on one node that is unable to run on other nodes. For example, linking off of Skylake modules may lead to problems when running on older Haswell nodes. If you try to compile using a set of incompatible modules, bad things can happen. At best, Cmake/Autotools will tell you to use something else; At worst, you can struggle with nebulous compiler errors for hours on end. At runtime the result is largely the same. From the Lmod documentation : it is quite easy to load an incompatible set of modules. Now, it is possible that the system administrators at your site might have set up conflicts to avoid loading mismatched modules. However, using conflicts can be fragile. What happens if a site adds a new compiler such as clang or pgi or a new mpi stack? All those module file conflict statements will have to be updated. A module hierarchy is a robust and elegant solution to avoid module conflicts. What is going on under the hood? \u00b6 Our current module tree has two main directories with many modules inside. $ echo $MODULEPATH /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64:/modules/modulefiles $ ls /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64 | wc -l 343 We are simply splitting it into multiple directories. /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64 \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 intelel/2021.4/ \u251c\u2500\u2500 intel-oneapi-mpi/2021.6.0-ad5zrqt/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 openblas/0.3.18-cuu4pwk/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 openmpi/4.1.3-lih7mwq/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 intel-mkl/2020.4.304-w2r5zyv/ \u251c\u2500\u2500 gcc/9.4.0/ The Unity module set is split in two. /modules/modulefiles contains our homebrew modules, those compiled by hand by the admins. /modules/spack/share/spack/modules/ contains modules created by Spack. /modules/modulefiles is not changing. Certain modules will now add a directory to your modulepath, making the modules in that directory available to load. At the start of each new login shell, only modules built with the system compiler gcc/9.4.0 will be included in the modulepath, as defined in the system-wide login scripts . Also included in this change \u00b6 Every new Slurm job will module purge . You will have to make sure that your jobs load modules all by themselves. This is to ensure that incompatible modules are not used. New commands are available in your shell: unity-module-find unity-module-hierarchy unity-module-hierarchy-help unity-module-lmod-disable-help unity-module-lmod-enable-help unity-slurm-list-constraints unity-slurm-find-nodes We are switching Spack TCL modules to Lua. This shouldn't affect the user experience. We are wrapping the module command with a bit of code which will print a message that things have changed. unity-module-lmod-disable-help is provided to suppress this output. Learn more \u00b6 Using the Module Hierarchy https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy https://lmod.readthedocs.io/en/latest/080_hierarchy.html","title":"The Module Hierarchy Change (Coming soon!) #"},{"location":"updates/hierarchy-change.html#the-module-hierarchy-change-coming-soon","text":"As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. The hierarchy change is to combat module bloat, simplify module names, and (most importantly) prevent conflicts. For many users, nothing will change, but some will have to take a few extra steps when loading modules going forward. The module command refers to Environment Modules. We use Lmod , which is Environment Modules implemented in Lua. The modulepath environment variable $MODULEPATH is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default. This means not all modules can be found with module avail by default. The information on this page is not critical for proper usage of Unity. We know your time is valuable, so you can always skip to Using the Module Hierarchy to learn how to work with these changes.","title":"The Module Hierarchy Change (Coming soon!)"},{"location":"updates/hierarchy-change.html#what-does-this-mean-for-my-workflow","text":"Without extra steps you may encounter the following error: No module(s) or extension(s) found! If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". In this case you can follow the documentation on how to use the hierarchy . In short, you use the unity-module-find command, and from that output you should be able to tell which other modules need to be loaded first. In many cases, it can be as simple as changing module load gromacs into module load openmpi gromacs .","title":"What does this mean for my workflow?"},{"location":"updates/hierarchy-change.html#does-this-affect-me","text":"The following modules are no longer in the default modulepath: modules built with the intel classic compilers zlib/1.2.12-intel@2021.4.0 pigz modules that use mpi cgns fftw/3.3.8+openmpi4.1.3 fftw/3.3.10+intel-oneapi-mpi2021.6.0 grace gromacs med mmg netcdf openfoam valgrind modules built for specific CPU micro-architectures cascadelake haswell icelake skylake_avx512 zen zen2","title":"Does this affect me?"},{"location":"updates/hierarchy-change.html#why","text":"","title":"Why?"},{"location":"updates/hierarchy-change.html#module-name-length","text":"Historically we have tried to make each module name unique. For example: fftw/3.3.8+openmpi4.1.3 fftw/3.3.8+intel-oneapi-mpi2021.6.0 This has worked well enough, but long module names make it harder for Lmod to display many modules on one screen, because horizontal whitespace can be used for more columns. This module: /modules/.../linux-ubuntu20.04-x86_64/ gromacs/2021.3+openmpi4.1.3-intel@2021.4 would instead become: /modules/.../linux-ubuntu20.04-x86_64/gcc/9.4.0/openmpi/4.1.3/intel/2021.4/ gromacs/2021.3","title":"Module name length"},{"location":"updates/hierarchy-change.html#module-name-conflicts","text":"If module names are not unique, it can be difficult to choose which module to load. When two modules from different directories have the same name, Lmod will decide which is default and mark it with a (D) flag. The only way to get around this is to manually manipulate $MODULEPATH to exclude certain directories until the desired module is marked as default. With the hierarchy, it's implicit that if you add special modules to your modulepath, you want those special modules to take priority.","title":"Module name conflicts"},{"location":"updates/hierarchy-change.html#module-conflicts","text":"Some software (particularly that which uses mpi) needs to be built in a similar manner to its dependencies. NetCDF, for example, will not work unless HDF5 and MPI are built with the same compiler and with certain parameters. With modules specific to CPU micro-architectures, it's possible to compile software on one node that is unable to run on other nodes. For example, linking off of Skylake modules may lead to problems when running on older Haswell nodes. If you try to compile using a set of incompatible modules, bad things can happen. At best, Cmake/Autotools will tell you to use something else; At worst, you can struggle with nebulous compiler errors for hours on end. At runtime the result is largely the same. From the Lmod documentation : it is quite easy to load an incompatible set of modules. Now, it is possible that the system administrators at your site might have set up conflicts to avoid loading mismatched modules. However, using conflicts can be fragile. What happens if a site adds a new compiler such as clang or pgi or a new mpi stack? All those module file conflict statements will have to be updated. A module hierarchy is a robust and elegant solution to avoid module conflicts.","title":"Module Conflicts"},{"location":"updates/hierarchy-change.html#what-is-going-on-under-the-hood","text":"Our current module tree has two main directories with many modules inside. $ echo $MODULEPATH /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64:/modules/modulefiles $ ls /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64 | wc -l 343 We are simply splitting it into multiple directories. /modules/spack/share/spack/modules/linux-ubuntu20.04-x86_64 \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 intelel/2021.4/ \u251c\u2500\u2500 intel-oneapi-mpi/2021.6.0-ad5zrqt/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 openblas/0.3.18-cuu4pwk/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 openmpi/4.1.3-lih7mwq/ \u251c\u2500\u2500 gcc/9.4.0/ \u251c\u2500\u2500 intel-mkl/2020.4.304-w2r5zyv/ \u251c\u2500\u2500 gcc/9.4.0/ The Unity module set is split in two. /modules/modulefiles contains our homebrew modules, those compiled by hand by the admins. /modules/spack/share/spack/modules/ contains modules created by Spack. /modules/modulefiles is not changing. Certain modules will now add a directory to your modulepath, making the modules in that directory available to load. At the start of each new login shell, only modules built with the system compiler gcc/9.4.0 will be included in the modulepath, as defined in the system-wide login scripts .","title":"What is going on under the hood?"},{"location":"updates/hierarchy-change.html#also-included-in-this-change","text":"Every new Slurm job will module purge . You will have to make sure that your jobs load modules all by themselves. This is to ensure that incompatible modules are not used. New commands are available in your shell: unity-module-find unity-module-hierarchy unity-module-hierarchy-help unity-module-lmod-disable-help unity-module-lmod-enable-help unity-slurm-list-constraints unity-slurm-find-nodes We are switching Spack TCL modules to Lua. This shouldn't affect the user experience. We are wrapping the module command with a bit of code which will print a message that things have changed. unity-module-lmod-disable-help is provided to suppress this output.","title":"Also included in this change"},{"location":"updates/hierarchy-change.html#learn-more","text":"Using the Module Hierarchy https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy https://lmod.readthedocs.io/en/latest/080_hierarchy.html","title":"Learn more"},{"location":"uploading-files/index.html","text":"Uploading Files to the Unity Filesystem \u00b6 The only way to add files to the Unity filesystem is through an SSL encrypted connection. It can be done with FileZilla, Globus, or in the command line. FileZilla is the recommended method for most because it strikes a balance between user friendliness and customizability. Globus is recommended for those who already have access to other Globus resources. scp and rsync are recommended for those who are comfortable in the command line and want to work quickly. Note Uploading files using American residential internet is typically very slow. UMass Amherst has a fibre line going directly to MGHPCC to improve speeds. Your Key File \u00b6 When you set up your Unity account, you chose between PuTTY ( .ppk ) and OpenSSH. ( .rsa ) scp and rsync use OpenSSH, and FileZilla prefers .ppk but can work with .rsa . Depending on which software you use, you can generate one of each. You can also convert between these keys using a program like PuttyGEN. Configuring SSH Keys Account Settings FileZilla \u00b6 FileZilla can use either an .rsa or a .ppk private key, but the 'Browse' button will show only .ppk files. To use an .rsa key, type in the path to the keyfile by hand. This guide assumes that your key lives at ~/.ssh/KEYFILE , but you can substitute this path. You can install FileZilla here FileZilla may ask you if you want to install McAfee, you probably don't. If you don't have antivirus already, you probably should. The FileZilla installer executable can be sometimes marked as a virus, it isn't. Select the Site Manager in FileZilla: Create a New Site: Fill in the Fields: Type a name for the site under My Sites on the left Protocol: SFTP Host: unity.rc.umass.edu User: your email but replace the . and @ with _ Key File: /path/to/your/keyfile This configuration is saved automatically. You can use the 'Connect' button in the bottom right to open an explorer on the Unity Filesystem,and you can drag and drop your files across the two panels. Properly connected, FileZilla should look like this: Globus \u00b6 Globus Connect allows for transfers between Globus collections. This can be useful, for example, when migrating from one HPC cluster to another. How can I transfer files to and from my local machine with Globus? Using Globus Connect in your browser \u00b6 Know which two Globus collections you want to transfer between. One is presumably Unity. The other could be your local machine if you install Globus Connect Personal. (see above) Go to app.globus.org If prompted, select your university and login with their identity provider. Go to the File Manager and select a collection. Either collection involved in your transfer will do. At the time of this writing, there is more than one collection named Unity. The following string can be pasted into the search box to select our Globus endpoint: acda5457-9c06-4564-8375-260ba428f22a Once a collection is selected, there should be two mirrored panels. If not, select the split panel layout in the top right: Select the other collection involved in your transfer. This will take you back to the collection search page. Of the two split panels, each has a Collection, a Path , a number of selected files, and a Start button to copy the selected files to the other side. Below is one panel: Configure your transfer and press 'Start'. CLI \u00b6 It's best to try this after you have already successfully connected to Unity with OpenSSH. As these are CLI procedures, the first thing you need to do is open your terminal and navigate to the directory (folder) where the files you want to upload are located. Alternatively you can use absolute paths in your command and skip this step. # Windows cd C:/Users/YOUR_NAME/Desktop # Linux cd /home/$USER/Desktop # Mac cd /Users/YOUR_NAME/Desktop Assuming, of course, that the files you want to upload are located in your desktop directory. And in the Windows case, assuming that the drive you want to copy from is the C drive. Note If your file name contains spaces, you will have to put it in quotes. SCP \u00b6 OpenSSH comes with the scp command, which uses the same argument structure as cp (copy) but with the added benefit of referencing the OpenSSH config file ( ~/.ssh/config ). This is how I can use unity as part of a command, because the OpenSSH config file contains the connection information for host unity . # single file scp FILE_NAME unity:~ # entire directory scp -r DIRECTORY_NAME unity:~ This will copy the files in question to your Unity home directory. You could also upload to elsewhere on the Unity filesystem, wherever you have permissions. Note -r in many commands is short for 'recursive'. It means to recursively open directories to ensure that all contained files are accounted for. Note ~ in the terminal represents your home directory. This is C:/Users/YOUR_NAME in Windows, /home/YOUR_NAME in Linux, and /Users/YOUR_NAME in Mac . RSYNC \u00b6 rsync can be installed on Linux and Mac. The syntax is the same as scp . It also references the OpenSSH config file. It's recommended to use the -tlp flags so that timestamps, relative links, and permissions are preserved, respectively. # single file rsync -tlp FILE_NAME unity:~ # entire directory rsync -rtlp DIRECTORY_NAME unity:~","title":"Uploading Files"},{"location":"uploading-files/index.html#uploading-files-to-the-unity-filesystem","text":"The only way to add files to the Unity filesystem is through an SSL encrypted connection. It can be done with FileZilla, Globus, or in the command line. FileZilla is the recommended method for most because it strikes a balance between user friendliness and customizability. Globus is recommended for those who already have access to other Globus resources. scp and rsync are recommended for those who are comfortable in the command line and want to work quickly. Note Uploading files using American residential internet is typically very slow. UMass Amherst has a fibre line going directly to MGHPCC to improve speeds.","title":"Uploading Files to the Unity Filesystem"},{"location":"uploading-files/index.html#your-key-file","text":"When you set up your Unity account, you chose between PuTTY ( .ppk ) and OpenSSH. ( .rsa ) scp and rsync use OpenSSH, and FileZilla prefers .ppk but can work with .rsa . Depending on which software you use, you can generate one of each. You can also convert between these keys using a program like PuttyGEN. Configuring SSH Keys Account Settings","title":"Your Key File"},{"location":"uploading-files/index.html#filezilla","text":"FileZilla can use either an .rsa or a .ppk private key, but the 'Browse' button will show only .ppk files. To use an .rsa key, type in the path to the keyfile by hand. This guide assumes that your key lives at ~/.ssh/KEYFILE , but you can substitute this path. You can install FileZilla here FileZilla may ask you if you want to install McAfee, you probably don't. If you don't have antivirus already, you probably should. The FileZilla installer executable can be sometimes marked as a virus, it isn't. Select the Site Manager in FileZilla: Create a New Site: Fill in the Fields: Type a name for the site under My Sites on the left Protocol: SFTP Host: unity.rc.umass.edu User: your email but replace the . and @ with _ Key File: /path/to/your/keyfile This configuration is saved automatically. You can use the 'Connect' button in the bottom right to open an explorer on the Unity Filesystem,and you can drag and drop your files across the two panels. Properly connected, FileZilla should look like this:","title":"FileZilla"},{"location":"uploading-files/index.html#globus","text":"Globus Connect allows for transfers between Globus collections. This can be useful, for example, when migrating from one HPC cluster to another. How can I transfer files to and from my local machine with Globus?","title":"Globus"},{"location":"uploading-files/index.html#using-globus-connect-in-your-browser","text":"Know which two Globus collections you want to transfer between. One is presumably Unity. The other could be your local machine if you install Globus Connect Personal. (see above) Go to app.globus.org If prompted, select your university and login with their identity provider. Go to the File Manager and select a collection. Either collection involved in your transfer will do. At the time of this writing, there is more than one collection named Unity. The following string can be pasted into the search box to select our Globus endpoint: acda5457-9c06-4564-8375-260ba428f22a Once a collection is selected, there should be two mirrored panels. If not, select the split panel layout in the top right: Select the other collection involved in your transfer. This will take you back to the collection search page. Of the two split panels, each has a Collection, a Path , a number of selected files, and a Start button to copy the selected files to the other side. Below is one panel: Configure your transfer and press 'Start'.","title":"Using Globus Connect in your browser"},{"location":"uploading-files/index.html#cli","text":"It's best to try this after you have already successfully connected to Unity with OpenSSH. As these are CLI procedures, the first thing you need to do is open your terminal and navigate to the directory (folder) where the files you want to upload are located. Alternatively you can use absolute paths in your command and skip this step. # Windows cd C:/Users/YOUR_NAME/Desktop # Linux cd /home/$USER/Desktop # Mac cd /Users/YOUR_NAME/Desktop Assuming, of course, that the files you want to upload are located in your desktop directory. And in the Windows case, assuming that the drive you want to copy from is the C drive. Note If your file name contains spaces, you will have to put it in quotes.","title":"CLI"},{"location":"uploading-files/index.html#scp","text":"OpenSSH comes with the scp command, which uses the same argument structure as cp (copy) but with the added benefit of referencing the OpenSSH config file ( ~/.ssh/config ). This is how I can use unity as part of a command, because the OpenSSH config file contains the connection information for host unity . # single file scp FILE_NAME unity:~ # entire directory scp -r DIRECTORY_NAME unity:~ This will copy the files in question to your Unity home directory. You could also upload to elsewhere on the Unity filesystem, wherever you have permissions. Note -r in many commands is short for 'recursive'. It means to recursively open directories to ensure that all contained files are accounted for. Note ~ in the terminal represents your home directory. This is C:/Users/YOUR_NAME in Windows, /home/YOUR_NAME in Linux, and /Users/YOUR_NAME in Mac .","title":"SCP"},{"location":"uploading-files/index.html#rsync","text":"rsync can be installed on Linux and Mac. The syntax is the same as scp . It also references the OpenSSH config file. It's recommended to use the -tlp flags so that timestamps, relative links, and permissions are preserved, respectively. # single file rsync -tlp FILE_NAME unity:~ # entire directory rsync -rtlp DIRECTORY_NAME unity:~","title":"RSYNC"}]}